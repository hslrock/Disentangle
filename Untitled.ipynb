{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model list=[Encoder,Discriminator_z, Discriminator_x,Generator,Phi,InvPhi]\n",
    "\n",
    "def joint_train():\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            \n",
    "            noi = noise_vector(real_images.size(0))\n",
    "\n",
    "            # 1-1.Train DiscriminatorX with img\n",
    "            fake_data = Decoder(noi).detach()\n",
    "            d_error, d_pred_real, d_pred_fake =gen_image_loss(True,real_images.float(), \n",
    "                                                              fake_data,Discriminator_reconstruct,\n",
    "                                                              dr_optimizer,d_optimizer)\n",
    "            # Train 1-2 GeneratorX\n",
    "            fake_data = Decoder(noi)#noise(real_batch.size(0)))\n",
    "            g_error = gen_image_loss(False, real_images.float(), fake_data,\n",
    "                                    Discriminator_reconstruct,\n",
    "                                    dr_optimizer,d_optimizer)\n",
    "\n",
    "            \n",
    "            #2 Train DiscriminatorZ\n",
    "            real_feature=Encoder(real_images.float()).detach()\n",
    "            df_error_adv=adv_feature_loss(True,noi,real_feature,\n",
    "                                          Discriminator_feature,\n",
    "                                          df_optimizer,e_optimizer)\n",
    "            \n",
    "            \n",
    "            real_feature=Encoder(real_images.float())\n",
    "            en_error_adv=adv_feature_loss(False,noi,real_feature,\n",
    "                                          Discriminator_feature,\n",
    "                                          df_optimizer,e_optimizer)\n",
    "            \n",
    "            \n",
    "            ######################################################\n",
    "            \n",
    "            \n",
    "             #Reconstruction Loss\n",
    "            z_tilde=invphi(torch.cat((glass_vector,gender_vector,remain),1))\n",
    "            loss_reconstruction=reconstruction_loss(latent_vector,z_tilde,opt_transform)\n",
    "            \n",
    "            \n",
    "            #Task Loss\n",
    "            opt_phi.zero_grad()       \n",
    "            loss=TripleletLoss(glass_vector,glasses) + TripleletLoss(gender_vector,gender)  \n",
    "            loss.backward(retain_graph=True)\n",
    "            opt_phi.step()\n",
    "\n",
    "            \n",
    "            #Cyclic Loss\n",
    "            loss_cycle=cyclic_loss(glass_vector,gender_vector,remain,glasses,gender,opt_transform)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ##############################################################\n",
    "            #3-1 Train DiscriminatorX_adv\n",
    "            \n",
    "            fake_data=Decoder(invphi(phi(Encoder(real_images.float())))).detach()\n",
    "            d_error1=adv_img_loss(True,real_images.float(),fake_data,\n",
    "                                  Discriminator_reconstruct,dr_optimizer,g_optimizer)\n",
    "            #3-2 Train GeneratorX_adv\n",
    "            fake_data=Decoder(invphi(phi(Encoder(real_images.float()))))\n",
    "            g_error2=adv_img_loss(True,real_images.float(),fake_data,\n",
    "                                  Discriminator_reconstruct,dr_optimizer,g_optimizer)\n",
    "            # 4. Train Reconstruction\n",
    "            \n",
    "            recons_loss=reconstruction_loss(g_optimizer,real_images.float(),Generator)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            latent_vector=Encoder(real_images).detach()\n",
    "            glass_vector,gender_vector,remain=phi(latent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test(a,b):\n",
    "    return a+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs,Encoder,Decoder,Discriminator_reconstruct,Discriminator_feature,d_optimizer,dr_optimizer,g_optimizer,train_load):\n",
    "    t_start = time.time()\n",
    "    discrim_error=[]\n",
    "    generator_error=[]\n",
    "    recons_error=[]\n",
    "    feature_discrim_error=[]\n",
    "    encoder_error=[]\n",
    "    discrim_adv_error=[]\n",
    "    duration_avg = 0.0\n",
    "    Encoder.train()\n",
    "    Decoder.train()\n",
    "    Generator.train()\n",
    "    Discriminator_reconstruct.train()\n",
    "    Discriminator_feature.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):        \n",
    " \n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            noi = noise_vector(real_images.size(0))\n",
    "\n",
    "            # 1-1.Train DiscriminatorX with img\n",
    "            fake_data = Decoder(noi).detach()\n",
    "            d_error, d_pred_real, d_pred_fake =gen_image_loss(True,real_images.float(), \n",
    "                                                              fake_data,Discriminator_reconstruct,\n",
    "                                                              dr_optimizer,d_optimizer)\n",
    "            # Train 1-2 GeneratorX\n",
    "            fake_data = Decoder(noi)#noise(real_batch.size(0)))\n",
    "            g_error = gen_image_loss(False, real_images.float(), fake_data,\n",
    "                                    Discriminator_reconstruct,\n",
    "                                    dr_optimizer,d_optimizer)\n",
    "\n",
    "            #2 Train DiscriminatorZ\n",
    "            real_feature=Encoder(real_images.float()).detach()\n",
    "            df_error_adv=adv_feature_loss(True,noi,real_feature,\n",
    "                                          Discriminator_feature,\n",
    "                                          df_optimizer,e_optimizer)\n",
    "            \n",
    "            \n",
    "            real_feature=Encoder(real_images.float())\n",
    "            en_error_adv=adv_feature_loss(False,noi,real_feature,\n",
    "                                          Discriminator_feature,\n",
    "                                          df_optimizer,e_optimizer)\n",
    "            \n",
    "            \n",
    "            #3-1 Train DiscriminatorX_adv\n",
    "            \n",
    "            fake_data=Generator(real_images.float()).detach()\n",
    "            d_error1=adv_img_loss(True,real_images.float(),fake_data,\n",
    "                                  Discriminator_reconstruct,dr_optimizer,g_optimizer)\n",
    "            #3-2 Train GeneratorX_adv\n",
    "            fake_data=Generator(real_images.float())\n",
    "            g_error2=adv_img_loss(True,real_images.float(),fake_data,\n",
    "                                  Discriminator_reconstruct,dr_optimizer,g_optimizer)\n",
    "            # 4. Train Reconstruction\n",
    "            \n",
    "            recons_loss=reconstruction_loss(g_optimizer,real_images.float(),Generator)\n",
    "            \n",
    "            \n",
    "            # Display Progress\n",
    "            if (batch_i) % 300 == 0:\n",
    "                print(\"Batch: \", batch_i)\n",
    "                print(\"1:Discriminator_Error: \", d_error.item(),\" Generator_Error: \", g_error.item(),\" Recons_Error: \", recons_loss.item())\n",
    "                print(\"2:Feature Discriminator Error: \",df_error_adv.item(),\"Encoder Error: \", en_error_adv.item())\n",
    "                print(\"3 Discriminator_adv_error\", d_error1.item(), \"Generator_error: \", g_error2.item())\n",
    "                discrim_error.append(d_error.item())\n",
    "                generator_error.append((float(g_error.item())+float(g_error2.item()))/2)\n",
    "                recons_error.append(recons_loss.item())\n",
    "                feature_discrim_error.append(df_error_adv.item())\n",
    "                encoder_error.append(en_error_adv.item())\n",
    "                discrim_adv_error.append(d_error1.item())\n",
    "            \n",
    "    \n",
    "        t_end = time.time()\n",
    "        duration_avg = (t_end - t_start) / (epoch + 1.0)\n",
    "        print(\"Elapsed Time: \",duration_avg)\n",
    "        torch.save(Encoder,'Encoder_64batch.h')\n",
    "        torch.save(Decoder,'Decoder_64batch.h')\n",
    "        torch.save(Discriminator_feature,'Discriminator_feature_64batch.h')\n",
    "        torch.save(Discriminator_reconstruct,'Discriminator_reconstruct_64batch.h')\n",
    "        eval_generate(Decoder,8)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Encoder,phi,invphi,train_load,num_epochs=40):\n",
    "    loss_matrix=None\n",
    "    first=True\n",
    "    t_start = time.time()\n",
    "    Encoder.eval()\n",
    "    Decoder.eval()\n",
    "    phi.train()\n",
    "    invphi.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            latent_vector=Encoder(real_images).detach()\n",
    "            glass_vector,gender_vector,remain=phi(latent_vector)\n",
    "            \n",
    "            #Reconstruction Loss\n",
    "            z_tilde=invphi(torch.cat((glass_vector,gender_vector,remain),1))\n",
    "            loss_reconstruction=reconstruction_loss(latent_vector,z_tilde,opt_transform)\n",
    "            \n",
    "            \n",
    "            #Task Loss\n",
    "            opt_phi.zero_grad()       \n",
    "            loss=TripleletLoss(glass_vector,glasses) +    TripleletLoss(gender_vector,gender)  \n",
    "            loss.backward(retain_graph=True)\n",
    "            opt_phi.step()\n",
    "\n",
    "            \n",
    "            #Cyclic Loss\n",
    "            loss_cycle=cyclic_loss(glass_vector,gender_vector,remain,glasses,gender,opt_transform)\n",
    "            \n",
    "            \n",
    "            if (batch_i) % 300 == 0:\n",
    "                print(\"Batch: \", batch_i)\n",
    "                print(\"Task Loss: \", loss.item())\n",
    "                print(\"Reconstruction Loss: \",loss_reconstruction.item())\n",
    "                print(\"Cyclic Loss: \",loss_cycle.item())\n",
    "                if first:\n",
    "                    loss_matrix=np.array((loss.item(),loss_reconstruction.item(),loss_cycle.item()))\n",
    "                    first=False\n",
    "                else:\n",
    "                    loss_matrix=np.vstack((loss_matrix,np.array((loss.item(),loss_reconstruction.item(),loss_cycle.item()))))\n",
    "        t_end = time.time()\n",
    "        duration_avg = (t_end - t_start) / (epoch + 1.0)\n",
    "        print(\"Elapsed Time: \",duration_avg)\n",
    "        torch.save(phi,'Phi.h')\n",
    "        torch.save(invphi,'invphi.h')\n",
    "    return loss_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_target(size):\n",
    "    data = torch.ones(size, 1, 8, 8)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_data_target(size):\n",
    "    data = torch.zeros(size, 1, 8, 8)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def real_feature_target(size):\n",
    "    data = torch.ones(size,1)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_feature_target(size):\n",
    "    data = torch.zeros(size,1)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def reconstruction_loss(optimizer,real_data,model_list):\n",
    "    reconstruction=Decoder(invphi(phi(Encoder(real_data))))\n",
    "    loss = nn.L1Loss()\n",
    "    optimizer.zero_grad()\n",
    "    error_recons=loss(real_data,reconstruction)*0.9\n",
    "    error_recons.backward()\n",
    "    optimizer.step()\n",
    "    return error_recons\n",
    "def adv_feature_loss(minimizing,real_feature,fake_feature,Discriminator,optimizer1,optimizer2):\n",
    "    loss=nn.BCELoss()\n",
    "    if minimizing:\n",
    "        optimizer1.zero_grad()\n",
    "        prediction_real = Discriminator(real_feature)\n",
    "        error_real = loss(prediction_real, real_feature_target(real_feature.size(0)))\n",
    "        error_real.backward()   \n",
    "    \n",
    "    \n",
    "        prediction_fake = Discriminator(fake_feature)\n",
    "        error_fake = loss(prediction_fake, fake_feature_target(real_feature.size(0)))\n",
    "        error_fake.backward()\n",
    "        total_error=error_real+error_fake\n",
    "        optimizer1.step()    \n",
    "        return total_error\n",
    "    else:\n",
    "        optimizer2.zero_grad()\n",
    "        prediction_fake=Discriminator(fake_feature)\n",
    "        error_fake = loss(prediction_fake, real_feature_target(real_feature.size(0)))\n",
    "        error_fake.backward()\n",
    "        optimizer2.step()    \n",
    "        return error_fake\n",
    "\n",
    "\n",
    "def adv_img_loss(minimizing,real_data,fake_data,Discriminator,optimizer1,optimizer2):\n",
    "    loss = nn.BCELoss()\n",
    "    def dloss_calc_adv(optimizer, real_data, fake_data,Discriminator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction_real = Discriminator(real_data)\n",
    "        error_real = loss(prediction_real, real_data_target(real_data.size(0)))\n",
    "        error_real.backward()\n",
    "        prediction_fake = Discriminator(fake_data)\n",
    "        error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))\n",
    "        error_fake.backward()\n",
    "        optimizer.step()\n",
    "        return error_real + error_fake\n",
    "    def gloss_calc_adv(optimizer, real_data,fake_data,Discriminator):\n",
    "        optimizer.zero_grad()\n",
    "        prediction_fake=Discriminator(fake_data_target)\n",
    "        error_fake = loss(prediction_fake, real_data_target(real_data.size(0)))\n",
    "        error_fake.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "    if minimizing:\n",
    "        return dloss_calc_adv(optimizer1,real_data,fake_data,Discriminator)\n",
    "    else:\n",
    "        return gloss_calc_adv(optimizer2,real_data,fake_data,Discriminator)\n",
    "    \n",
    "    \n",
    "def gen_image_loss(minimizing,real_data, fake_data,Discriminator, optimizer1,optimizer2,weight=0.8):\n",
    "    def dloss_calc(optimizer, real_data, fake_data,Discriminator):\n",
    "        optimizer.zero_grad()\n",
    "        prediction_real = Discriminator(real_data)\n",
    "        error_real = loss(prediction_real, real_data_target(real_data.size(0)))*weight\n",
    "        error_real.backward()\n",
    "        prediction_fake = Discriminator(fake_data)\n",
    "        error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))*weight\n",
    "        error_fake.backward()\n",
    "        optimizer.step()\n",
    "        return error_real + error_fake, prediction_real, prediction_fake\n",
    "    def gloss_calc(optimizer, fake_data,Discriminator):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = Discriminator(fake_data)\n",
    "        error = loss(prediction, real_data_target(prediction.size(0)))*weight\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "        return error\n",
    "    \n",
    "    loss = nn.BCELoss()\n",
    "        \n",
    "    if minimizing:\n",
    "        return dloss_calc(optimizer1,real_data,fake_data,Discriminator)\n",
    "    else:\n",
    "        return gloss_calc(optimizer2,fake_data,Discriminator)\n",
    "\n",
    "def noise_vector(size):\n",
    "    n = torch.randn(size,99)\n",
    "    if torch.cuda.is_available(): return n.cuda() \n",
    "    return n\n",
    "\n",
    "def eval_generate(Decoder,num_images):\n",
    "    Decoder.eval()\n",
    "    noi_input=noise_vector(num_images)\n",
    "    output=Decoder(noi_input)\n",
    "    output=output.detach().cpu()\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    grid_border_size = 2\n",
    "    grid = utils.make_grid(output)\n",
    "    \n",
    "    plt.imshow((grid.numpy().transpose((1, 2, 0))*0.5)+0.5)\n",
    "    plt.axis('off')\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "def train(num_epochs,Encoder,Decoder,Generator,Discriminator_reconstruct,Discriminator_feature,d_optimizer,dr_optimizer,g_optimizer,train_load):\n",
    "    t_start = time.time()\n",
    "    discrim_error=[]\n",
    "    generator_error=[]\n",
    "    recons_error=[]\n",
    "    feature_discrim_error=[]\n",
    "    encoder_error=[]\n",
    "    discrim_adv_error=[]\n",
    "    duration_avg = 0.0\n",
    "    Encoder.train()\n",
    "    Decoder.train()\n",
    "    Generator.train()\n",
    "    Discriminator_reconstruct.train()\n",
    "    Discriminator_feature.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):        \n",
    " \n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            noi = noise_vector(real_images.size(0))\n",
    "\n",
    "            # 1-1.Train DiscriminatorX with img\n",
    "            fake_data = Decoder(noi).detach()\n",
    "            d_error, d_pred_real, d_pred_fake =gen_image_loss(True,real_images.float(), \n",
    "                                                              fake_data,Discriminator_reconstruct,\n",
    "                                                              dr_optimizer,d_optimizer)\n",
    "            # Train 1-2 GeneratorX\n",
    "            fake_data = Decoder(noi)#noise(real_batch.size(0)))\n",
    "            g_error = gen_image_loss(False, real_images.float(), fake_data,\n",
    "                                    Discriminator_reconstruct,\n",
    "                                    dr_optimizer,d_optimizer)\n",
    "\n",
    "            #2 Train DiscriminatorZ\n",
    "            \n",
    "            real_feature=Encoder(real_images.float()).detach()\n",
    "            df_error_adv=adv_feature_loss(True,noi,real_feature,\n",
    "                                          Discriminator_feature,\n",
    "                                          df_optimizer,e_optimizer)\n",
    "            \n",
    "            \n",
    "            real_feature=Encoder(real_images.float())\n",
    "            en_error_adv=adv_feature_loss(False,noi,real_feature,\n",
    "                                          Discriminator_feature,\n",
    "                                          df_optimizer,e_optimizer)\n",
    "            \n",
    "            \n",
    "            #3-1 Train DiscriminatorX_adv\n",
    "            \n",
    "            fake_data=Generator(real_images.float()).detach()\n",
    "            d_error1=adv_img_loss(True,real_images.float(),fake_data,\n",
    "                                  Discriminator_reconstruct,dr_optimizer,g_optimizer)\n",
    "            #3-2 Train GeneratorX_adv\n",
    "            fake_data=Generator(real_images.float())\n",
    "            g_error2=adv_img_loss(True,real_images.float(),fake_data,\n",
    "                                  Discriminator_reconstruct,dr_optimizer,g_optimizer)\n",
    "            # 4. Train Reconstruction\n",
    "            \n",
    "            recons_loss=reconstruction_loss(g_optimizer,real_images.float(),Generator)\n",
    "            \n",
    "            \n",
    "            # Display Progress\n",
    "            if (batch_i) % 300 == 0:\n",
    "                print(\"Batch: \", batch_i)\n",
    "                print(\"1:Discriminator_Error: \", d_error.item(),\" Generator_Error: \", g_error.item(),\" Recons_Error: \", recons_loss.item())\n",
    "                print(\"2:Feature Discriminator Error: \",df_error_adv.item(),\"Encoder Error: \", en_error_adv.item())\n",
    "                print(\"3 Discriminator_adv_error\", d_error1.item(), \"Generator_error: \", g_error2.item())\n",
    "                discrim_error.append(d_error.item())\n",
    "                generator_error.append((float(g_error.item())+float(g_error2.item()))/2)\n",
    "                recons_error.append(recons_loss.item())\n",
    "                feature_discrim_error.append(df_error_adv.item())\n",
    "                encoder_error.append(en_error_adv.item())\n",
    "                discrim_adv_error.append(d_error1.item())\n",
    "            \n",
    "    \n",
    "        t_end = time.time()\n",
    "        duration_avg = (t_end - t_start) / (epoch + 1.0)\n",
    "        print(\"Elapsed Time: \",duration_avg)\n",
    "        torch.save(Encoder,'Encoder_64batch.h')\n",
    "        torch.save(Decoder,'Decoder_64batch.h')\n",
    "        torch.save(Discriminator_feature,'Discriminator_feature_64batch.h')\n",
    "        torch.save(Discriminator_reconstruct,'Discriminator_reconstruct_64batch.h')\n",
    "        eval_generate(Decoder,8)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
