{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import EncoderNet,DecoderNet,DiscriminatorNet_reconstruction,GeneratorNet\n",
    "from network import transformNet\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "#import resnet\n",
    "#import invresnet\n",
    "from dataload import load_data ,batchfy \n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms, utils\n",
    "TIMEOUT=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Decoder_64batch.h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1b96be7abc14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Encoder_64batch.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDecoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Decoder_64batch.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Encoder=resnet.resnet18()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Encoder=Encoder.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Decoder_64batch.h'"
     ]
    }
   ],
   "source": [
    "Encoder=torch.load(\"Encoder_64batch.h\")\n",
    "Decoder=torch.load(\"Decoder_64batch.h\")\n",
    "#Encoder=resnet.resnet18()\n",
    "#Encoder=Encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load,test_load=batchfy(batch_size=100)\n",
    "show_img=iter(train_load)\n",
    "for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "    debug=real_images[0]\n",
    "    plt.imshow((debug.numpy().transpose((1, 2, 0))*0.5)+0.5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phi(\n",
       "  (fc1): Linear(in_features=99, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=99, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi=transformNet.phi()\n",
    "phi.to(device)\n",
    "\n",
    "invphi=transformNet.phi(inv=True)\n",
    "invphi.to(device)\n",
    "\n",
    "#Transform=transformNet.full_phi(phi,invphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_phi = optim.Adam(phi.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "opt_invphi = optim.Adam(invphi.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [phi.parameters(), invphi.parameters()]\n",
    "\n",
    "opt_transform=optim.Adam(itertools.chain(*params),lr=0.001,betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripleletLoss(batch,targetAttribute):\n",
    "    def triplet(value, positive, negative, margin=0.2) : \n",
    "        d = nn.PairwiseDistance(p=2)\n",
    "        distance = d(value, positive) - d(value, negative) + margin \n",
    "        loss = torch.mean(torch.max(distance, torch.zeros_like(distance))) \n",
    "        return loss\n",
    "    \n",
    "    def findtriplet(src,attribute):\n",
    "        timeout_start = time.time()\n",
    "        index_list=np.arange(len(attribute)).tolist()\n",
    "        rand=random.sample(index_list,len(attribute))\n",
    "        for i,posindex in enumerate(rand):\n",
    "            if attribute[src]==attribute[posindex]:\n",
    "                if src != posindex:\n",
    "                        break      \n",
    "            if i==len(attribute)-1:\n",
    "                posindex=src            \n",
    "        rand=random.sample(index_list,len(attribute))                \n",
    "        for i,negindex in enumerate(rand):\n",
    "            if(attribute[src] !=attribute[negindex]):\n",
    "                break   \n",
    "            if i==len(attribute)-1:\n",
    "                negindex=src\n",
    "                \n",
    "        return posindex,negindex\n",
    "    loss=0\n",
    "    pos_pair=None\n",
    "    for i,value in enumerate(batch):\n",
    "        posindex,negindex=findtriplet(i,targetAttribute)\n",
    "\n",
    "        if not i:\n",
    "\n",
    "            pos_pair=batch[posindex].unsqueeze(0)\n",
    "            neg_pair=batch[negindex].unsqueeze(0)\n",
    "        else:\n",
    "            pos_pair=torch.cat((pos_pair,batch[posindex].unsqueeze(0)),0)\n",
    "            neg_pair=torch.cat((neg_pair,batch[negindex].unsqueeze(0)),0)\n",
    "\n",
    "\n",
    "    return triplet(batch,pos_pair,neg_pair)\n",
    "\n",
    "def reconstruction_loss(z,z_tilde,optimizer):\n",
    "    loss = nn.L1Loss()\n",
    "    optimizer.zero_grad()\n",
    "    error_recons=loss(z,z_tilde)\n",
    "    error_recons.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return error_recons\n",
    "\n",
    "def concat(z_list):\n",
    "    return torch.cat((z_list[0],z_list[1],z_list[2]),1)\n",
    "\n",
    "def cyclic_loss(z1,z2,z3,true_glasses,true_gender,opt_transform):\n",
    "    batch_size=z1.size(0)\n",
    "    swapped_pos=torch.randperm(batch_size)   \n",
    "    z1_hat = z1[swapped_pos]   #Permutation\n",
    "    true_glasses=true_glasses[swapped_pos]  \n",
    "    swapped_pos=torch.randperm(batch_size)\n",
    "    true_gender=true_glasses[swapped_pos]\n",
    "    z2_hat=z2[swapped_pos]\n",
    "    swapped_pos=torch.randperm(batch_size)\n",
    "    z3_hat=z3[swapped_pos]\n",
    "    true_gender=true_glasses[true_glasses]\n",
    "    z_aster=torch.cat((z1_hat,z2_hat,z3),1)\n",
    "    recontructed_z_aster=concat(phi(Encoder(Decoder(invphi(z_aster)))))\n",
    "    \n",
    "\n",
    "    \n",
    "    #Cycle_Consistency,Loss                 \n",
    "    opt_transform.zero_grad()\n",
    "    loss = nn.MSELoss()                                                     \n",
    "    consistency_loss = loss(z_aster,recontructed_z_aster)\n",
    "    consistency_loss.backward(retain_graph=True)\n",
    "    opt_transform.step()\n",
    "    \n",
    "    \n",
    "    #attr_cycle_augmentation_loss\n",
    "    opt_transform.zero_grad()\n",
    "    augmentation_loss =TripleletLoss(z1_hat,true_glasses) + TripleletLoss(z2_hat,true_gender)\n",
    "    augmentation_loss.backward()\n",
    "    opt_transform.step()\n",
    " \n",
    "    return consistency_loss +augmentation_loss       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [

    "def train(Encoder,phi,invphi,train_load,num_epochs=40):\n",
    "    loss_matrix=None\n",
    "    first=True\n",
>>>>>>> 718235a810c6422927929ce9f8c4a490516ea543
    "    t_start = time.time()\n",
    "    Encoder.eval()\n",
    "    Decoder.eval()\n",
    "    phi.train()\n",
    "    invphi.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            latent_vector=Encoder(real_images).detach()\n",
    "            glass_vector,gender_vector,remain=phi(latent_vector)\n",
    "            \n",
    "            #Reconstruction Loss\n",
    "            z_tilde=invphi(torch.cat((glass_vector,gender_vector,remain),1))\n",
    "            loss_reconstruction=reconstruction_loss(latent_vector,z_tilde,opt_transform)\n",
    "            \n",
    "            \n",
    "            #Task Loss\n",
    "            opt_phi.zero_grad()       \n",
    "            loss=TripleletLoss(glass_vector,glasses) +    TripleletLoss(gender_vector,gender)  \n",
    "            loss.backward(retain_graph=True)\n",
    "            opt_phi.step()\n",
    "            \n",
    "            #glass_vector=glass_vector.detach()\n",
    "           # gender_vector=gender_vector.detach()\n",
    "           # remain=remain.detach()\n",
    "\n",
    "            \n",
    "            #Cyclic Loss\n",
    "            loss_cycle=cyclic_loss(glass_vector,gender_vector,remain,glasses,gender,opt_transform)\n",
    "            \n",
    "            \n",
    "            if (batch_i) % 300 == 0:\n",
    "                print(\"Batch: \", batch_i)\n",
    "                print(\"Task Loss: \", loss.item())\n",
    "                print(\"Reconstruction Loss: \",loss_reconstruction.item())\n",
    "                print(\"Cyclic Loss: \",loss_cycle.item())\n",
    "                if first:\n",
    "                    loss_matrix=np.array((loss.item(),loss_reconstruction.item(),loss_cycle.item()))\n",
    "                    first=False\n",
    "                else:\n",
    "                    loss_matrix=np.vstack((loss_matrix,np.array((loss.item(),loss_reconstruction.item(),loss_cycle.item()))))\n",
    "        t_end = time.time()\n",
    "        duration_avg = (t_end - t_start) / (epoch + 1.0)\n",
    "        print(\"Elapsed Time: \",duration_avg)\n",

    "        torch.save(phi,'Phi.h')\n",
    "        torch.save(invphi,'invphi.h')\n",
    "    return loss_matrix"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch:  0\n",
      "Task Loss:  0.41485169529914856\n",
      "Reconstruction Loss:  0.4043228328227997\n",
      "Cyclic Loss:  1.5709236860275269\n",
      "Batch:  300\n",
      "Task Loss:  0.32545140385627747\n",
      "Reconstruction Loss:  0.10201364755630493\n",
      "Cyclic Loss:  0.5533437132835388\n",
      "Batch:  600\n",
      "Task Loss:  0.30165576934814453\n",
      "Reconstruction Loss:  0.10666044056415558\n",
      "Cyclic Loss:  0.43101686239242554\n",
      "Batch:  900\n",
      "Task Loss:  0.22351883351802826\n",
      "Reconstruction Loss:  0.10000026971101761\n",
      "Cyclic Loss:  0.3561117649078369\n",
      "Batch:  1200\n",
      "Task Loss:  0.3101048171520233\n",
      "Reconstruction Loss:  0.10411772131919861\n",
      "Cyclic Loss:  0.4668482840061188\n",
      "Batch:  1500\n",
      "Task Loss:  0.29572728276252747\n",
      "Reconstruction Loss:  0.10546872764825821\n",
      "Cyclic Loss:  0.349165141582489\n",
      "Elapsed Time:  1570.3096861839294\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type phi. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Task Loss:  0.2934541702270508\n",
      "Reconstruction Loss:  0.10784789174795151\n",
      "Cyclic Loss:  0.46450483798980713\n",
      "Batch:  300\n",
      "Task Loss:  0.23126842081546783\n",
      "Reconstruction Loss:  0.09972812235355377\n",
      "Cyclic Loss:  0.31033873558044434\n",
      "Batch:  600\n",
      "Task Loss:  0.29877969622612\n",
      "Reconstruction Loss:  0.09143738448619843\n",
      "Cyclic Loss:  0.3066519498825073\n",
      "Batch:  900\n",
      "Task Loss:  0.27809929847717285\n",
      "Reconstruction Loss:  0.09516167640686035\n",
      "Cyclic Loss:  0.35675376653671265\n",
      "Batch:  1200\n",
      "Task Loss:  0.26769736409187317\n",
      "Reconstruction Loss:  0.10073316842317581\n",
      "Cyclic Loss:  0.42011886835098267\n",
      "Batch:  1500\n",
      "Task Loss:  0.29206162691116333\n",
      "Reconstruction Loss:  0.09946095943450928\n",
      "Cyclic Loss:  0.24799978733062744\n",
      "Elapsed Time:  1569.5283200740814\n",
      "Epoch: 2\n",
      "Batch:  0\n",
      "Task Loss:  0.2703222930431366\n",
      "Reconstruction Loss:  0.0904562696814537\n",
      "Cyclic Loss:  0.31055641174316406\n",
      "Batch:  300\n",
      "Task Loss:  0.2664741575717926\n",
      "Reconstruction Loss:  0.08968904614448547\n",
      "Cyclic Loss:  0.33835259079933167\n",
      "Batch:  600\n",
      "Task Loss:  0.26454028487205505\n",
      "Reconstruction Loss:  0.11322079598903656\n",
      "Cyclic Loss:  0.5295050144195557\n",
      "Batch:  900\n",
      "Task Loss:  0.21743862330913544\n",
      "Reconstruction Loss:  0.09974829107522964\n",
      "Cyclic Loss:  0.43126049637794495\n",
      "Batch:  1200\n",
      "Task Loss:  0.24467605352401733\n",
      "Reconstruction Loss:  0.09193921834230423\n",
      "Cyclic Loss:  0.2610914707183838\n",
      "Batch:  1500\n",
      "Task Loss:  0.2418588399887085\n",
      "Reconstruction Loss:  0.09427006542682648\n",
      "Cyclic Loss:  0.339705228805542\n",
      "Elapsed Time:  1576.4744562307994\n",
      "Epoch: 3\n",
      "Batch:  0\n",
      "Task Loss:  0.3236810564994812\n",
      "Reconstruction Loss:  0.08453822880983353\n",
      "Cyclic Loss:  0.3274063766002655\n",
      "Batch:  300\n",
      "Task Loss:  0.23834745585918427\n",
      "Reconstruction Loss:  0.08753841370344162\n",
      "Cyclic Loss:  0.2965394854545593\n",
      "Batch:  600\n",
      "Task Loss:  0.2471085637807846\n",
      "Reconstruction Loss:  0.09782013297080994\n",
      "Cyclic Loss:  0.38341212272644043\n",
      "Batch:  900\n",
      "Task Loss:  0.26681140065193176\n",
      "Reconstruction Loss:  0.09172187000513077\n",
      "Cyclic Loss:  0.34337252378463745\n",
      "Batch:  1200\n",
      "Task Loss:  0.27790942788124084\n",
      "Reconstruction Loss:  0.08707629889249802\n",
      "Cyclic Loss:  0.31561020016670227\n",
      "Batch:  1500\n",
      "Task Loss:  0.4012511074542999\n",
      "Reconstruction Loss:  0.1608409434556961\n",
      "Cyclic Loss:  0.9621202349662781\n",
      "Elapsed Time:  1578.2365495562553\n",
      "Epoch: 4\n",
      "Batch:  0\n",
      "Task Loss:  0.23541998863220215\n",
      "Reconstruction Loss:  0.08466091752052307\n",
      "Cyclic Loss:  0.26862165331840515\n",
      "Batch:  300\n",
      "Task Loss:  0.35048753023147583\n",
      "Reconstruction Loss:  0.11812318116426468\n",
      "Cyclic Loss:  0.5745354294776917\n",
      "Batch:  600\n",
      "Task Loss:  0.29838764667510986\n",
      "Reconstruction Loss:  0.08857832103967667\n",
      "Cyclic Loss:  0.34937772154808044\n",
      "Batch:  900\n",
      "Task Loss:  0.27876636385917664\n",
      "Reconstruction Loss:  0.08690991997718811\n",
      "Cyclic Loss:  0.28952041268348694\n",
      "Batch:  1200\n",
      "Task Loss:  0.27566248178482056\n",
      "Reconstruction Loss:  0.08816554397344589\n",
      "Cyclic Loss:  0.3400076627731323\n",
      "Batch:  1500\n",
      "Task Loss:  0.3225793242454529\n",
      "Reconstruction Loss:  0.08741609752178192\n",
      "Cyclic Loss:  0.2988104820251465\n",
      "Elapsed Time:  1575.341585111618\n",
      "Epoch: 5\n",
      "Batch:  0\n",
      "Task Loss:  0.2533304989337921\n",
      "Reconstruction Loss:  0.10903581976890564\n",
      "Cyclic Loss:  0.5581241250038147\n",
      "Batch:  300\n",
      "Task Loss:  0.26907819509506226\n",
      "Reconstruction Loss:  0.08352721482515335\n",
      "Cyclic Loss:  0.34064820408821106\n",
      "Batch:  600\n",
      "Task Loss:  0.2604994773864746\n",
      "Reconstruction Loss:  0.10409679263830185\n",
      "Cyclic Loss:  0.2781238555908203\n",
      "Batch:  900\n",
      "Task Loss:  0.24537846446037292\n",
      "Reconstruction Loss:  0.0859021544456482\n",
      "Cyclic Loss:  0.2784212827682495\n",
      "Batch:  1200\n",
      "Task Loss:  0.29180628061294556\n",
      "Reconstruction Loss:  0.0896877646446228\n",
      "Cyclic Loss:  0.4060460031032562\n",
      "Batch:  1500\n",
      "Task Loss:  0.26940053701400757\n",
      "Reconstruction Loss:  0.10357245802879333\n",
      "Cyclic Loss:  0.42689424753189087\n",
      "Elapsed Time:  1578.04711886247\n",
      "Epoch: 6\n",
      "Batch:  0\n",
      "Task Loss:  0.2547900080680847\n",
      "Reconstruction Loss:  0.08693521469831467\n",
      "Cyclic Loss:  0.3013344407081604\n",
      "Batch:  300\n",
      "Task Loss:  0.31309032440185547\n",
      "Reconstruction Loss:  0.08387365192174911\n",
      "Cyclic Loss:  0.3281097114086151\n",
      "Batch:  600\n",
      "Task Loss:  0.2989406883716583\n",
      "Reconstruction Loss:  0.1066598892211914\n",
      "Cyclic Loss:  0.4234801232814789\n",
      "Batch:  900\n",
      "Task Loss:  0.31490445137023926\n",
      "Reconstruction Loss:  0.08155510574579239\n",
      "Cyclic Loss:  0.28526005148887634\n",
      "Batch:  1200\n",
      "Task Loss:  0.25138694047927856\n",
      "Reconstruction Loss:  0.08548209071159363\n",
      "Cyclic Loss:  0.30265942215919495\n",
      "Batch:  1500\n",
      "Task Loss:  0.27096474170684814\n",
      "Reconstruction Loss:  0.09916459769010544\n",
      "Cyclic Loss:  0.4114244878292084\n",
      "Elapsed Time:  1576.4791998182025\n",
      "Epoch: 7\n",
      "Batch:  0\n",
      "Task Loss:  0.27287623286247253\n",
      "Reconstruction Loss:  0.10037808120250702\n",
      "Cyclic Loss:  0.4009259045124054\n",
      "Batch:  300\n",
      "Task Loss:  0.2510908544063568\n",
      "Reconstruction Loss:  0.09709589183330536\n",
      "Cyclic Loss:  0.40709036588668823\n",
      "Batch:  600\n",
      "Task Loss:  0.19621002674102783\n",
      "Reconstruction Loss:  0.10703279823064804\n",
      "Cyclic Loss:  0.3601485788822174\n",
      "Batch:  900\n",
      "Task Loss:  0.2809951603412628\n",
      "Reconstruction Loss:  0.09336381405591965\n",
      "Cyclic Loss:  0.4100351929664612\n",
      "Batch:  1200\n",
      "Task Loss:  0.22097228467464447\n",
      "Reconstruction Loss:  0.09617336839437485\n",
      "Cyclic Loss:  0.3553208112716675\n",
      "Batch:  1500\n",
      "Task Loss:  0.2378058284521103\n",
      "Reconstruction Loss:  0.09232481569051743\n",
      "Cyclic Loss:  0.3386535048484802\n",
      "Elapsed Time:  1575.6111213564873\n",
      "Epoch: 8\n",
      "Batch:  0\n",
      "Task Loss:  0.28810277581214905\n",
      "Reconstruction Loss:  0.09333808720111847\n",
      "Cyclic Loss:  0.4269362986087799\n",
      "Batch:  300\n",
      "Task Loss:  0.2675704061985016\n",
      "Reconstruction Loss:  0.08417532593011856\n",
      "Cyclic Loss:  0.3240911066532135\n",
      "Batch:  600\n",
      "Task Loss:  0.251505970954895\n",
      "Reconstruction Loss:  0.08453690260648727\n",
      "Cyclic Loss:  0.2657924294471741\n",
      "Batch:  900\n",
      "Task Loss:  0.24931037425994873\n",
      "Reconstruction Loss:  0.0885128304362297\n",
      "Cyclic Loss:  0.35901373624801636\n",
      "Batch:  1200\n",
      "Task Loss:  0.28619712591171265\n",
      "Reconstruction Loss:  0.08307049423456192\n",
      "Cyclic Loss:  0.331760436296463\n",
      "Batch:  1500\n",
      "Task Loss:  0.2846151888370514\n",
      "Reconstruction Loss:  0.0971960574388504\n",
      "Cyclic Loss:  0.3544314205646515\n",
      "Elapsed Time:  1575.5499287976158\n",
      "Epoch: 9\n",
      "Batch:  0\n",
      "Task Loss:  0.24478107690811157\n",
      "Reconstruction Loss:  0.0859985500574112\n",
      "Cyclic Loss:  0.281216025352478\n",
      "Batch:  300\n",
      "Task Loss:  0.28967034816741943\n",
      "Reconstruction Loss:  0.0865798145532608\n",
      "Cyclic Loss:  0.3882908821105957\n",
      "Batch:  600\n",
      "Task Loss:  0.2621324956417084\n",
      "Reconstruction Loss:  0.08199954032897949\n",
      "Cyclic Loss:  0.2643093168735504\n",
      "Batch:  900\n",
      "Task Loss:  0.28241127729415894\n",
      "Reconstruction Loss:  0.08762335032224655\n",
      "Cyclic Loss:  0.3216143250465393\n",
      "Batch:  1200\n",
      "Task Loss:  0.2930676341056824\n",
      "Reconstruction Loss:  0.08129880577325821\n",
      "Cyclic Loss:  0.33612722158432007\n",
      "Batch:  1500\n",
      "Task Loss:  0.3478158712387085\n",
      "Reconstruction Loss:  0.09265827387571335\n",
      "Cyclic Loss:  0.46754369139671326\n",
      "Elapsed Time:  1575.9818890333177\n",
      "Epoch: 10\n",
      "Batch:  0\n",
      "Task Loss:  0.2396884262561798\n",
      "Reconstruction Loss:  0.08183678984642029\n",
      "Cyclic Loss:  0.2823222875595093\n",
      "Batch:  300\n",
      "Task Loss:  0.2550230026245117\n",
      "Reconstruction Loss:  0.08188093453645706\n",
      "Cyclic Loss:  0.30039548873901367\n",
      "Batch:  600\n",
      "Task Loss:  0.2720339000225067\n",
      "Reconstruction Loss:  0.08852209150791168\n",
      "Cyclic Loss:  0.32509762048721313\n",
      "Batch:  900\n",
      "Task Loss:  0.30729734897613525\n",
      "Reconstruction Loss:  0.08381429314613342\n",
      "Cyclic Loss:  0.3531390130519867\n",
      "Batch:  1200\n",
      "Task Loss:  0.27110278606414795\n",
      "Reconstruction Loss:  0.09253504872322083\n",
      "Cyclic Loss:  0.44807755947113037\n",
      "Batch:  1500\n",
      "Task Loss:  0.2843848764896393\n",
      "Reconstruction Loss:  0.0871039554476738\n",
      "Cyclic Loss:  0.2923243045806885\n",
      "Elapsed Time:  1575.3440902883356\n",
      "Epoch: 11\n",
      "Batch:  0\n",
      "Task Loss:  0.2660485804080963\n",
      "Reconstruction Loss:  0.08489134162664413\n",
      "Cyclic Loss:  0.27840301394462585\n",
      "Batch:  300\n",
      "Task Loss:  0.35284295678138733\n",
      "Reconstruction Loss:  0.12846198678016663\n",
      "Cyclic Loss:  0.7858054041862488\n",
      "Batch:  600\n",
      "Task Loss:  0.2571754455566406\n",
      "Reconstruction Loss:  0.08375462889671326\n",
      "Cyclic Loss:  0.2798757255077362\n",
      "Batch:  900\n",
      "Task Loss:  0.28231415152549744\n",
      "Reconstruction Loss:  0.08510740101337433\n",
      "Cyclic Loss:  0.29872754216194153\n",
      "Batch:  1200\n",
      "Task Loss:  0.21092252433300018\n",
      "Reconstruction Loss:  0.08754455298185349\n",
      "Cyclic Loss:  0.26843541860580444\n",
      "Batch:  1500\n",
      "Task Loss:  0.23739424347877502\n",
      "Reconstruction Loss:  0.08380022644996643\n",
      "Cyclic Loss:  0.2652801275253296\n",
      "Elapsed Time:  1574.8206182916958\n",
      "Epoch: 12\n",
      "Batch:  0\n",
      "Task Loss:  0.26371029019355774\n",
      "Reconstruction Loss:  0.09164940565824509\n",
      "Cyclic Loss:  0.3378591537475586\n",
      "Batch:  300\n",
      "Task Loss:  0.26685795187950134\n",
      "Reconstruction Loss:  0.09978143125772476\n",
      "Cyclic Loss:  0.4374581575393677\n",
      "Batch:  600\n",
      "Task Loss:  0.23659902811050415\n",
      "Reconstruction Loss:  0.08913964778184891\n",
      "Cyclic Loss:  0.3067419230937958\n",
      "Batch:  900\n",
      "Task Loss:  0.27175745368003845\n",
      "Reconstruction Loss:  0.08615254610776901\n",
      "Cyclic Loss:  0.300476998090744\n",
      "Batch:  1200\n",
      "Task Loss:  0.34127306938171387\n",
      "Reconstruction Loss:  0.25018373131752014\n",
      "Cyclic Loss:  0.7729863524436951\n",
      "Batch:  1500\n",
      "Task Loss:  0.22821581363677979\n",
      "Reconstruction Loss:  0.08068519830703735\n",
      "Cyclic Loss:  0.27962636947631836\n",
      "Elapsed Time:  1575.7571271382844\n",
      "Epoch: 13\n",
      "Batch:  0\n",
      "Task Loss:  0.2221604883670807\n",
      "Reconstruction Loss:  0.08451702445745468\n",
      "Cyclic Loss:  0.26976272463798523\n",
      "Batch:  300\n",
      "Task Loss:  0.2893393039703369\n",
      "Reconstruction Loss:  0.09310988336801529\n",
      "Cyclic Loss:  0.3468439280986786\n",
      "Batch:  600\n",
      "Task Loss:  0.2976187467575073\n",
      "Reconstruction Loss:  0.12290828675031662\n",
      "Cyclic Loss:  0.7027063369750977\n",
      "Batch:  900\n",
      "Task Loss:  0.2777111530303955\n",
      "Reconstruction Loss:  0.08126981556415558\n",
      "Cyclic Loss:  0.30965670943260193\n",
      "Batch:  1200\n",
      "Task Loss:  0.27204829454421997\n",
      "Reconstruction Loss:  0.10035216808319092\n",
      "Cyclic Loss:  0.49542924761772156\n",
      "Batch:  1500\n",
      "Task Loss:  0.3413058817386627\n",
      "Reconstruction Loss:  0.08647909015417099\n",
      "Cyclic Loss:  0.40226197242736816\n",
      "Elapsed Time:  1575.9091711725507\n",
      "Epoch: 14\n",
      "Batch:  0\n",
      "Task Loss:  0.3367842733860016\n",
      "Reconstruction Loss:  0.09502465277910233\n",
      "Cyclic Loss:  0.4630599319934845\n",
      "Batch:  300\n",
      "Task Loss:  0.23684130609035492\n",
      "Reconstruction Loss:  0.08596321195363998\n",
      "Cyclic Loss:  0.27228039503097534\n",
      "Batch:  600\n",
      "Task Loss:  0.30743134021759033\n",
      "Reconstruction Loss:  0.08672424405813217\n",
      "Cyclic Loss:  0.36206942796707153\n",
      "Batch:  900\n",
      "Task Loss:  0.24128329753875732\n",
      "Reconstruction Loss:  0.08361665904521942\n",
      "Cyclic Loss:  0.2638618052005768\n",
      "Batch:  1200\n",
      "Task Loss:  0.2590855062007904\n",
      "Reconstruction Loss:  0.08404222875833511\n",
      "Cyclic Loss:  0.2968539595603943\n",
      "Batch:  1500\n",
      "Task Loss:  0.2322750687599182\n",
      "Reconstruction Loss:  0.08546729385852814\n",
      "Cyclic Loss:  0.27874886989593506\n",
      "Elapsed Time:  1575.7780847708384\n",
      "Epoch: 15\n",
      "Batch:  0\n",
      "Task Loss:  0.2816847264766693\n",
      "Reconstruction Loss:  0.09345906972885132\n",
      "Cyclic Loss:  0.32703912258148193\n",
      "Batch:  300\n",
      "Task Loss:  0.2721431255340576\n",
      "Reconstruction Loss:  0.09474662691354752\n",
      "Cyclic Loss:  0.3812416195869446\n",
      "Batch:  600\n",
      "Task Loss:  0.23945499956607819\n",
      "Reconstruction Loss:  0.08062715083360672\n",
      "Cyclic Loss:  0.27597030997276306\n",
      "Batch:  900\n",
      "Task Loss:  0.27930381894111633\n",
      "Reconstruction Loss:  0.08240467309951782\n",
      "Cyclic Loss:  0.2882349491119385\n",
      "Batch:  1200\n",
      "Task Loss:  0.2230754792690277\n",
      "Reconstruction Loss:  0.08631667494773865\n",
      "Cyclic Loss:  0.2822865843772888\n",
      "Batch:  1500\n",
      "Task Loss:  0.26269978284835815\n",
      "Reconstruction Loss:  0.0808497816324234\n",
      "Cyclic Loss:  0.30640333890914917\n",
      "Elapsed Time:  1575.8472322970629\n",
      "Epoch: 16\n",
      "Batch:  0\n",
      "Task Loss:  0.2782377600669861\n",
      "Reconstruction Loss:  0.08532258868217468\n",
      "Cyclic Loss:  0.3587803840637207\n",
      "Batch:  300\n",
      "Task Loss:  0.27640217542648315\n",
      "Reconstruction Loss:  0.08269120007753372\n",
      "Cyclic Loss:  0.3385137915611267\n",
      "Batch:  600\n",
      "Task Loss:  0.2526191473007202\n",
      "Reconstruction Loss:  0.13372200727462769\n",
      "Cyclic Loss:  0.6196611523628235\n",
      "Batch:  900\n",
      "Task Loss:  0.22824323177337646\n",
      "Reconstruction Loss:  0.08203662931919098\n",
      "Cyclic Loss:  0.2441723495721817\n",
      "Batch:  1200\n",
      "Task Loss:  0.21642336249351501\n",
      "Reconstruction Loss:  0.092130646109581\n",
      "Cyclic Loss:  0.29957956075668335\n",
      "Batch:  1500\n",
      "Task Loss:  0.31006166338920593\n",
      "Reconstruction Loss:  0.10477069765329361\n",
      "Cyclic Loss:  0.46957242488861084\n",
      "Elapsed Time:  1575.7898387067457\n",
      "Epoch: 17\n",
      "Batch:  0\n",
      "Task Loss:  0.25938916206359863\n",
      "Reconstruction Loss:  0.08494575321674347\n",
      "Cyclic Loss:  0.2881632149219513\n",
      "Batch:  300\n",
      "Task Loss:  0.2685384750366211\n",
      "Reconstruction Loss:  0.08106422424316406\n",
      "Cyclic Loss:  0.2805120050907135\n",
      "Batch:  600\n",
      "Task Loss:  0.25599342584609985\n",
      "Reconstruction Loss:  0.11235850304365158\n",
      "Cyclic Loss:  0.4827459752559662\n",
      "Batch:  900\n",
      "Task Loss:  0.22104573249816895\n",
      "Reconstruction Loss:  0.08468605577945709\n",
      "Cyclic Loss:  0.23116810619831085\n",
      "Batch:  1200\n",
      "Task Loss:  0.2522980272769928\n",
      "Reconstruction Loss:  0.08486250787973404\n",
      "Cyclic Loss:  0.2921048402786255\n",
      "Batch:  1500\n",
      "Task Loss:  0.21791547536849976\n",
      "Reconstruction Loss:  0.0977480337023735\n",
      "Cyclic Loss:  0.44776344299316406\n",
      "Elapsed Time:  1576.0078430175781\n",
      "Epoch: 18\n",
      "Batch:  0\n",
      "Task Loss:  0.3097069263458252\n",
      "Reconstruction Loss:  0.08266300708055496\n",
      "Cyclic Loss:  0.40089040994644165\n",
      "Batch:  300\n",
      "Task Loss:  0.23755429685115814\n",
      "Reconstruction Loss:  0.08158823847770691\n",
      "Cyclic Loss:  0.2683970034122467\n",
      "Batch:  600\n",
      "Task Loss:  0.257686972618103\n",
      "Reconstruction Loss:  0.08871902525424957\n",
      "Cyclic Loss:  0.25189027190208435\n",
      "Batch:  900\n",
      "Task Loss:  0.2720121443271637\n",
      "Reconstruction Loss:  0.08331090956926346\n",
      "Cyclic Loss:  0.27802181243896484\n",
      "Batch:  1200\n",
      "Task Loss:  0.27136844396591187\n",
      "Reconstruction Loss:  0.0815795287489891\n",
      "Cyclic Loss:  0.32364779710769653\n",
      "Batch:  1500\n",
      "Task Loss:  0.2718968391418457\n",
      "Reconstruction Loss:  0.08230409026145935\n",
      "Cyclic Loss:  0.30833950638771057\n",
      "Elapsed Time:  1576.7711526971113\n",
      "Epoch: 19\n",
      "Batch:  0\n",
      "Task Loss:  0.25430187582969666\n",
      "Reconstruction Loss:  0.10140839964151382\n",
      "Cyclic Loss:  0.4200877845287323\n",
      "Batch:  300\n",
      "Task Loss:  0.27948662638664246\n",
      "Reconstruction Loss:  0.08285142481327057\n",
      "Cyclic Loss:  0.2938142716884613\n",
      "Batch:  600\n",
      "Task Loss:  0.2717478275299072\n",
      "Reconstruction Loss:  0.08361998945474625\n",
      "Cyclic Loss:  0.30935609340667725\n",
      "Batch:  900\n",
      "Task Loss:  0.33089977502822876\n",
      "Reconstruction Loss:  0.13579678535461426\n",
      "Cyclic Loss:  0.38029104471206665\n",
      "Batch:  1200\n",
      "Task Loss:  0.30230623483657837\n",
      "Reconstruction Loss:  0.08215353637933731\n",
      "Cyclic Loss:  0.3579576015472412\n",
      "Batch:  1500\n",
      "Task Loss:  0.29801687598228455\n",
      "Reconstruction Loss:  0.08355394750833511\n",
      "Cyclic Loss:  0.3627307713031769\n",
      "Elapsed Time:  1576.8239812374115\n",
      "Epoch: 20\n",
      "Batch:  0\n",
      "Task Loss:  0.3525660037994385\n",
      "Reconstruction Loss:  0.08026827126741409\n",
      "Cyclic Loss:  0.38517674803733826\n",
      "Batch:  300\n",
      "Task Loss:  0.2571331560611725\n",
      "Reconstruction Loss:  0.08058038353919983\n",
      "Cyclic Loss:  0.2756195366382599\n",
      "Batch:  600\n",
      "Task Loss:  0.2542687952518463\n",
      "Reconstruction Loss:  0.10025104880332947\n",
      "Cyclic Loss:  0.3804459273815155\n",
      "Batch:  900\n",
      "Task Loss:  0.2646714448928833\n",
      "Reconstruction Loss:  0.08860403299331665\n",
      "Cyclic Loss:  0.3278971314430237\n",
      "Batch:  1200\n",
      "Task Loss:  0.2594393789768219\n",
      "Reconstruction Loss:  0.08483466506004333\n",
      "Cyclic Loss:  0.32251331210136414\n",
      "Batch:  1500\n",
      "Task Loss:  0.2802216112613678\n",
      "Reconstruction Loss:  0.1031966432929039\n",
      "Cyclic Loss:  0.24966467916965485\n",
      "Elapsed Time:  1576.1495861780077\n",
      "Epoch: 21\n",
      "Batch:  0\n",
      "Task Loss:  0.3096317946910858\n",
      "Reconstruction Loss:  0.1234234943985939\n",
      "Cyclic Loss:  0.33982017636299133\n",
      "Batch:  300\n",
      "Task Loss:  0.2589585781097412\n",
      "Reconstruction Loss:  0.0839521735906601\n",
      "Cyclic Loss:  0.285839319229126\n",
      "Batch:  600\n",
      "Task Loss:  0.26112303137779236\n",
      "Reconstruction Loss:  0.08179294317960739\n",
      "Cyclic Loss:  0.2670802175998688\n",
      "Batch:  900\n",
      "Task Loss:  0.24338844418525696\n",
      "Reconstruction Loss:  0.09938228130340576\n",
      "Cyclic Loss:  0.41068536043167114\n",
      "Batch:  1200\n",
      "Task Loss:  0.3446720242500305\n",
      "Reconstruction Loss:  0.07897648215293884\n",
      "Cyclic Loss:  0.3905525803565979\n",
      "Batch:  1500\n",
      "Task Loss:  0.35296565294265747\n",
      "Reconstruction Loss:  0.0998011976480484\n",
      "Cyclic Loss:  0.5516841411590576\n",
      "Elapsed Time:  1575.784407214685\n",
      "Epoch: 22\n",
      "Batch:  0\n",
      "Task Loss:  0.2661552429199219\n",
      "Reconstruction Loss:  0.08822353184223175\n",
      "Cyclic Loss:  0.2670450508594513\n",
      "Batch:  300\n",
      "Task Loss:  0.22836096584796906\n",
      "Reconstruction Loss:  0.0895165205001831\n",
      "Cyclic Loss:  0.3049054741859436\n",
      "Batch:  600\n",
      "Task Loss:  0.24084284901618958\n",
      "Reconstruction Loss:  0.0838385745882988\n",
      "Cyclic Loss:  0.30239927768707275\n",
      "Batch:  900\n",
      "Task Loss:  0.270574152469635\n",
      "Reconstruction Loss:  0.09083904325962067\n",
      "Cyclic Loss:  0.2850857377052307\n",
      "Batch:  1200\n",
      "Task Loss:  0.2987600564956665\n",
      "Reconstruction Loss:  0.11565215140581131\n",
      "Cyclic Loss:  0.546825110912323\n",
      "Batch:  1500\n",
      "Task Loss:  0.27372944355010986\n",
      "Reconstruction Loss:  0.0960693284869194\n",
      "Cyclic Loss:  0.31711331009864807\n",
      "Elapsed Time:  1575.6862565641818\n",
      "Epoch: 23\n",
      "Batch:  0\n",
      "Task Loss:  0.260077565908432\n",
      "Reconstruction Loss:  0.08720554411411285\n",
      "Cyclic Loss:  0.2992536425590515\n",
      "Batch:  300\n",
      "Task Loss:  0.23388642072677612\n",
      "Reconstruction Loss:  0.09872101247310638\n",
      "Cyclic Loss:  0.4087015986442566\n",
      "Batch:  600\n",
      "Task Loss:  0.3230668902397156\n",
      "Reconstruction Loss:  0.08031716197729111\n",
      "Cyclic Loss:  0.3525899350643158\n",
      "Batch:  900\n",
      "Task Loss:  0.24043774604797363\n",
      "Reconstruction Loss:  0.09303496778011322\n",
      "Cyclic Loss:  0.40831080079078674\n",
      "Batch:  1200\n",
      "Task Loss:  0.27829432487487793\n",
      "Reconstruction Loss:  0.08425496518611908\n",
      "Cyclic Loss:  0.33422526717185974\n",
      "Batch:  1500\n",
      "Task Loss:  0.23172351717948914\n",
      "Reconstruction Loss:  0.08769119530916214\n",
      "Cyclic Loss:  0.2806844413280487\n",
      "Elapsed Time:  1575.3331062893074\n",
      "Epoch: 24\n",
      "Batch:  0\n",
      "Task Loss:  0.27100542187690735\n",
      "Reconstruction Loss:  0.0869871973991394\n",
      "Cyclic Loss:  0.2980198860168457\n",
      "Batch:  300\n",
      "Task Loss:  0.2836316227912903\n",
      "Reconstruction Loss:  0.08663209527730942\n",
      "Cyclic Loss:  0.3222682774066925\n",
      "Batch:  600\n",
      "Task Loss:  0.29582515358924866\n",
      "Reconstruction Loss:  0.09564033895730972\n",
      "Cyclic Loss:  0.37550610303878784\n",
      "Batch:  900\n",
      "Task Loss:  0.25375521183013916\n",
      "Reconstruction Loss:  0.0874742940068245\n",
      "Cyclic Loss:  0.39372050762176514\n",
      "Batch:  1200\n",
      "Task Loss:  0.26342788338661194\n",
      "Reconstruction Loss:  0.08153501898050308\n",
      "Cyclic Loss:  0.269479900598526\n",
      "Batch:  1500\n",
      "Task Loss:  0.2896273136138916\n",
      "Reconstruction Loss:  0.08409209549427032\n",
      "Cyclic Loss:  0.2760956287384033\n",
      "Elapsed Time:  1574.6566074180603\n",
      "Epoch: 25\n",
      "Batch:  0\n",
      "Task Loss:  0.24141696095466614\n",
      "Reconstruction Loss:  0.11727184057235718\n",
      "Cyclic Loss:  0.49243849515914917\n",
      "Batch:  300\n",
      "Task Loss:  0.26735368371009827\n",
      "Reconstruction Loss:  0.08761164546012878\n",
      "Cyclic Loss:  0.36352312564849854\n",
      "Batch:  600\n",
      "Task Loss:  0.3203340768814087\n",
      "Reconstruction Loss:  0.08627081662416458\n",
      "Cyclic Loss:  0.40770262479782104\n",
      "Batch:  900\n",
      "Task Loss:  0.277726411819458\n",
      "Reconstruction Loss:  0.08684108406305313\n",
      "Cyclic Loss:  0.2904554009437561\n",
      "Batch:  1200\n",
      "Task Loss:  0.34651026129722595\n",
      "Reconstruction Loss:  0.09616237133741379\n",
      "Cyclic Loss:  0.4421621859073639\n",
      "Batch:  1500\n",
      "Task Loss:  0.26798656582832336\n",
      "Reconstruction Loss:  0.0952879935503006\n",
      "Cyclic Loss:  0.4495367407798767\n",
      "Elapsed Time:  1574.2020559219213\n",
      "Epoch: 26\n",
      "Batch:  0\n",
      "Task Loss:  0.24291732907295227\n",
      "Reconstruction Loss:  0.08508957922458649\n",
      "Cyclic Loss:  0.29190897941589355\n",
      "Batch:  300\n",
      "Task Loss:  0.27943941950798035\n",
      "Reconstruction Loss:  0.08966372907161713\n",
      "Cyclic Loss:  0.3269311785697937\n",
      "Batch:  600\n",
      "Task Loss:  0.2289399951696396\n",
      "Reconstruction Loss:  0.08857294917106628\n",
      "Cyclic Loss:  0.30272844433784485\n",
      "Batch:  900\n",
      "Task Loss:  0.2730516195297241\n",
      "Reconstruction Loss:  0.0821990966796875\n",
      "Cyclic Loss:  0.3063357472419739\n",
      "Batch:  1200\n",
      "Task Loss:  0.23846733570098877\n",
      "Reconstruction Loss:  0.08362366259098053\n",
      "Cyclic Loss:  0.2771068513393402\n",
      "Batch:  1500\n",
      "Task Loss:  0.32011348009109497\n",
      "Reconstruction Loss:  0.08934012055397034\n",
      "Cyclic Loss:  0.3953050971031189\n",
      "Elapsed Time:  1574.593706952201\n",
      "Epoch: 27\n",
      "Batch:  0\n",
      "Task Loss:  0.3030860424041748\n",
      "Reconstruction Loss:  0.08276837319135666\n",
      "Cyclic Loss:  0.31296926736831665\n",
      "Batch:  300\n",
      "Task Loss:  0.23673754930496216\n",
      "Reconstruction Loss:  0.09857647866010666\n",
      "Cyclic Loss:  0.4356597661972046\n",
      "Batch:  600\n",
      "Task Loss:  0.2944435477256775\n",
      "Reconstruction Loss:  0.0802907645702362\n",
      "Cyclic Loss:  0.3253633975982666\n",
      "Batch:  900\n",
      "Task Loss:  0.3078185021877289\n",
      "Reconstruction Loss:  0.08664266020059586\n",
      "Cyclic Loss:  0.3887861669063568\n",
      "Batch:  1200\n",
      "Task Loss:  0.2820275127887726\n",
      "Reconstruction Loss:  0.08535957336425781\n",
      "Cyclic Loss:  0.31380078196525574\n",
      "Batch:  1500\n",
      "Task Loss:  0.21314996480941772\n",
      "Reconstruction Loss:  0.11076880246400833\n",
      "Cyclic Loss:  0.4001937210559845\n",
      "Elapsed Time:  1574.169051851545\n",
      "Epoch: 28\n",
      "Batch:  0\n",
      "Task Loss:  0.24677756428718567\n",
      "Reconstruction Loss:  0.08804420381784439\n",
      "Cyclic Loss:  0.3445951044559479\n",
      "Batch:  300\n",
      "Task Loss:  0.26015567779541016\n",
      "Reconstruction Loss:  0.08888215571641922\n",
      "Cyclic Loss:  0.3070812225341797\n",
      "Batch:  600\n",
      "Task Loss:  0.28308749198913574\n",
      "Reconstruction Loss:  0.09527057409286499\n",
      "Cyclic Loss:  0.40982332825660706\n",
      "Batch:  900\n",
      "Task Loss:  0.20411479473114014\n",
      "Reconstruction Loss:  0.08175300806760788\n",
      "Cyclic Loss:  0.23469382524490356\n",
      "Batch:  1200\n",
      "Task Loss:  0.2793118953704834\n",
      "Reconstruction Loss:  0.08753150701522827\n",
      "Cyclic Loss:  0.4122077226638794\n",
      "Batch:  1500\n",
      "Task Loss:  0.21824049949645996\n",
      "Reconstruction Loss:  0.09370654821395874\n",
      "Cyclic Loss:  0.36101996898651123\n",
      "Elapsed Time:  1574.1036990511006\n",
      "Epoch: 29\n",
      "Batch:  0\n",
      "Task Loss:  0.2241288125514984\n",
      "Reconstruction Loss:  0.08239099383354187\n",
      "Cyclic Loss:  0.29710933566093445\n",
      "Batch:  300\n",
      "Task Loss:  0.29469597339630127\n",
      "Reconstruction Loss:  0.10219050198793411\n",
      "Cyclic Loss:  0.2888445556163788\n",
      "Batch:  600\n",
      "Task Loss:  0.22075454890727997\n",
      "Reconstruction Loss:  0.08205759525299072\n",
      "Cyclic Loss:  0.2510407865047455\n",
      "Batch:  900\n",
      "Task Loss:  0.22568565607070923\n",
      "Reconstruction Loss:  0.08651150017976761\n",
      "Cyclic Loss:  0.2490156590938568\n",
      "Batch:  1200\n",
      "Task Loss:  0.32486236095428467\n",
      "Reconstruction Loss:  0.09370038658380508\n",
      "Cyclic Loss:  0.3758862316608429\n",
      "Batch:  1500\n",
      "Task Loss:  0.2510119378566742\n",
      "Reconstruction Loss:  0.08679565787315369\n",
      "Cyclic Loss:  0.2946951389312744\n",
      "Elapsed Time:  1573.379077498118\n",
      "Epoch: 30\n",
      "Batch:  0\n",
      "Task Loss:  0.3127492368221283\n",
      "Reconstruction Loss:  0.101705402135849\n",
      "Cyclic Loss:  0.5581213235855103\n",
      "Batch:  300\n",
      "Task Loss:  0.27610155940055847\n",
      "Reconstruction Loss:  0.08254389464855194\n",
      "Cyclic Loss:  0.2964785695075989\n",
      "Batch:  600\n",
      "Task Loss:  0.2883598804473877\n",
      "Reconstruction Loss:  0.08374711126089096\n",
      "Cyclic Loss:  0.3244129419326782\n",
      "Batch:  900\n",
      "Task Loss:  0.3127516508102417\n",
      "Reconstruction Loss:  0.0898948460817337\n",
      "Cyclic Loss:  0.3825538456439972\n",
      "Batch:  1200\n",
      "Task Loss:  0.24646802246570587\n",
      "Reconstruction Loss:  0.09352072328329086\n",
      "Cyclic Loss:  0.39474961161613464\n",
      "Batch:  1500\n",
      "Task Loss:  0.2631891965866089\n",
      "Reconstruction Loss:  0.09123478829860687\n",
      "Cyclic Loss:  0.3352278769016266\n",
      "Elapsed Time:  1572.7317435049242\n",
      "Epoch: 31\n",
      "Batch:  0\n",
      "Task Loss:  0.2583788335323334\n",
      "Reconstruction Loss:  0.0836179181933403\n",
      "Cyclic Loss:  0.3289056718349457\n",
      "Batch:  300\n",
      "Task Loss:  0.2576826214790344\n",
      "Reconstruction Loss:  0.08083391934633255\n",
      "Cyclic Loss:  0.25550368428230286\n",
      "Batch:  600\n",
      "Task Loss:  0.27150580286979675\n",
      "Reconstruction Loss:  0.08956850320100784\n",
      "Cyclic Loss:  0.31212788820266724\n",
      "Batch:  900\n",
      "Task Loss:  0.25459811091423035\n",
      "Reconstruction Loss:  0.09551195800304413\n",
      "Cyclic Loss:  0.31854209303855896\n",
      "Batch:  1200\n",
      "Task Loss:  0.22723501920700073\n",
      "Reconstruction Loss:  0.11646629869937897\n",
      "Cyclic Loss:  0.549533486366272\n",
      "Batch:  1500\n",
      "Task Loss:  0.2376820296049118\n",
      "Reconstruction Loss:  0.08109129965305328\n",
      "Cyclic Loss:  0.27303358912467957\n",
      "Elapsed Time:  1573.0903252884746\n",
      "Epoch: 32\n",
      "Batch:  0\n",
      "Task Loss:  0.3681012690067291\n",
      "Reconstruction Loss:  0.0842413529753685\n",
      "Cyclic Loss:  0.36576443910598755\n",
      "Batch:  300\n",
      "Task Loss:  0.353655070066452\n",
      "Reconstruction Loss:  0.09081878513097763\n",
      "Cyclic Loss:  0.43051546812057495\n",
      "Batch:  600\n",
      "Task Loss:  0.2116481065750122\n",
      "Reconstruction Loss:  0.08246877789497375\n",
      "Cyclic Loss:  0.2510431408882141\n",
      "Batch:  900\n",
      "Task Loss:  0.27641284465789795\n",
      "Reconstruction Loss:  0.08762520551681519\n",
      "Cyclic Loss:  0.30800214409828186\n",
      "Batch:  1200\n",
      "Task Loss:  0.23756609857082367\n",
      "Reconstruction Loss:  0.08972768485546112\n",
      "Cyclic Loss:  0.3535521924495697\n",
      "Batch:  1500\n",
      "Task Loss:  0.29985368251800537\n",
      "Reconstruction Loss:  0.11663490533828735\n",
      "Cyclic Loss:  0.5649076700210571\n",
      "Elapsed Time:  1573.5540453592937\n",
      "Epoch: 33\n",
      "Batch:  0\n",
      "Task Loss:  0.23900260031223297\n",
      "Reconstruction Loss:  0.08422660827636719\n",
      "Cyclic Loss:  0.2914223372936249\n",
      "Batch:  300\n",
      "Task Loss:  0.317985475063324\n",
      "Reconstruction Loss:  0.08543068170547485\n",
      "Cyclic Loss:  0.3100627064704895\n",
      "Batch:  600\n",
      "Task Loss:  0.4500988721847534\n",
      "Reconstruction Loss:  0.09074398875236511\n",
      "Cyclic Loss:  0.5169540047645569\n",
      "Batch:  900\n",
      "Task Loss:  0.3027515411376953\n",
      "Reconstruction Loss:  0.0858524814248085\n",
      "Cyclic Loss:  0.3211081922054291\n",
      "Batch:  1200\n",
      "Task Loss:  0.27085936069488525\n",
      "Reconstruction Loss:  0.08479215204715729\n",
      "Cyclic Loss:  0.2833315134048462\n",
      "Batch:  1500\n",
      "Task Loss:  0.3339071273803711\n",
      "Reconstruction Loss:  0.09910847246646881\n",
      "Cyclic Loss:  0.4784826636314392\n",
      "Elapsed Time:  1573.1915224229588\n",
      "Epoch: 34\n",
      "Batch:  0\n",
      "Task Loss:  0.3145765960216522\n",
      "Reconstruction Loss:  0.10165087878704071\n",
      "Cyclic Loss:  0.4722316861152649\n",
      "Batch:  300\n",
      "Task Loss:  0.252642959356308\n",
      "Reconstruction Loss:  0.09120643138885498\n",
      "Cyclic Loss:  0.38413676619529724\n",
      "Batch:  600\n",
      "Task Loss:  0.2968369722366333\n",
      "Reconstruction Loss:  0.08789054304361343\n",
      "Cyclic Loss:  0.29255545139312744\n",
      "Batch:  900\n",
      "Task Loss:  0.29507559537887573\n",
      "Reconstruction Loss:  0.08810429275035858\n",
      "Cyclic Loss:  0.3073906898498535\n",
      "Batch:  1200\n",
      "Task Loss:  0.29106801748275757\n",
      "Reconstruction Loss:  0.08232565224170685\n",
      "Cyclic Loss:  0.302634060382843\n",
      "Batch:  1500\n",
      "Task Loss:  0.2264576554298401\n",
      "Reconstruction Loss:  0.09690559655427933\n",
      "Cyclic Loss:  0.3798399865627289\n",
      "Elapsed Time:  1572.8452309199743\n",
      "Epoch: 35\n",
      "Batch:  0\n",
      "Task Loss:  0.2495936155319214\n",
      "Reconstruction Loss:  0.09297022223472595\n",
      "Cyclic Loss:  0.34436389803886414\n",
      "Batch:  300\n",
      "Task Loss:  0.21222051978111267\n",
      "Reconstruction Loss:  0.0845261812210083\n",
      "Cyclic Loss:  0.26155027747154236\n",
      "Batch:  600\n",
      "Task Loss:  0.2663002610206604\n",
      "Reconstruction Loss:  0.08560387045145035\n",
      "Cyclic Loss:  0.2876514196395874\n",
      "Batch:  900\n",
      "Task Loss:  0.3151048421859741\n",
      "Reconstruction Loss:  0.11279284954071045\n",
      "Cyclic Loss:  0.6182919144630432\n",
      "Batch:  1200\n",
      "Task Loss:  0.28996405005455017\n",
      "Reconstruction Loss:  0.08548880368471146\n",
      "Cyclic Loss:  0.3405255973339081\n",
      "Batch:  1500\n",
      "Task Loss:  0.2828904688358307\n",
      "Reconstruction Loss:  0.0898047611117363\n",
      "Cyclic Loss:  0.3652108609676361\n",
      "Elapsed Time:  1572.6169819103347\n",
      "Epoch: 36\n",
      "Batch:  0\n",
      "Task Loss:  0.2632322311401367\n",
      "Reconstruction Loss:  0.09332534670829773\n",
      "Cyclic Loss:  0.3851410448551178\n",
      "Batch:  300\n",
      "Task Loss:  0.27163293957710266\n",
      "Reconstruction Loss:  0.08030426502227783\n",
      "Cyclic Loss:  0.30077043175697327\n",
      "Batch:  600\n",
      "Task Loss:  0.2141318917274475\n",
      "Reconstruction Loss:  0.0938805565237999\n",
      "Cyclic Loss:  0.38118550181388855\n",
      "Batch:  900\n",
      "Task Loss:  0.27107957005500793\n",
      "Reconstruction Loss:  0.08301596343517303\n",
      "Cyclic Loss:  0.31428027153015137\n",
      "Batch:  1200\n",
      "Task Loss:  0.2552695870399475\n",
      "Reconstruction Loss:  0.08677521347999573\n",
      "Cyclic Loss:  0.28438612818717957\n",
      "Batch:  1500\n",
      "Task Loss:  0.23870185017585754\n",
      "Reconstruction Loss:  0.08756831288337708\n",
      "Cyclic Loss:  0.32072150707244873\n",
      "Elapsed Time:  1572.7718263187924\n",
      "Epoch: 37\n",
      "Batch:  0\n",
      "Task Loss:  0.23429878056049347\n",
      "Reconstruction Loss:  0.0792284607887268\n",
      "Cyclic Loss:  0.282488614320755\n",
      "Batch:  300\n",
      "Task Loss:  0.31984254717826843\n",
      "Reconstruction Loss:  0.09968686103820801\n",
      "Cyclic Loss:  0.4537906050682068\n",
      "Batch:  600\n",
      "Task Loss:  0.27411288022994995\n",
      "Reconstruction Loss:  0.08660981059074402\n",
      "Cyclic Loss:  0.35890552401542664\n",
      "Batch:  900\n",
      "Task Loss:  0.3224538564682007\n",
      "Reconstruction Loss:  0.0812992975115776\n",
      "Cyclic Loss:  0.3336283266544342\n",
      "Batch:  1200\n",
      "Task Loss:  0.3023485839366913\n",
      "Reconstruction Loss:  0.10469946265220642\n",
      "Cyclic Loss:  0.5910220146179199\n",
      "Batch:  1500\n",
      "Task Loss:  0.26629820466041565\n",
      "Reconstruction Loss:  0.08230365067720413\n",
      "Cyclic Loss:  0.2970134913921356\n",
      "Elapsed Time:  1572.6945881027925\n",
      "Epoch: 38\n",
      "Batch:  0\n",
      "Task Loss:  0.2231736183166504\n",
      "Reconstruction Loss:  0.08391155302524567\n",
      "Cyclic Loss:  0.2521367371082306\n",
      "Batch:  300\n",
      "Task Loss:  0.2516844868659973\n",
      "Reconstruction Loss:  0.10184995085000992\n",
      "Cyclic Loss:  0.2531137764453888\n",
      "Batch:  600\n",
      "Task Loss:  0.3224128782749176\n",
      "Reconstruction Loss:  0.08272641897201538\n",
      "Cyclic Loss:  0.3524158298969269\n",
      "Batch:  900\n",
      "Task Loss:  0.24966734647750854\n",
      "Reconstruction Loss:  0.08829557150602341\n",
      "Cyclic Loss:  0.24584728479385376\n",
      "Batch:  1200\n",
      "Task Loss:  0.2461767941713333\n",
      "Reconstruction Loss:  0.08379870653152466\n",
      "Cyclic Loss:  0.28618910908699036\n",
      "Batch:  1500\n",
      "Task Loss:  0.23858101665973663\n",
      "Reconstruction Loss:  0.0850272923707962\n",
      "Cyclic Loss:  0.29328516125679016\n",
      "Elapsed Time:  1572.5757904052734\n",
      "Epoch: 39\n",
      "Batch:  0\n",
      "Task Loss:  0.23966321349143982\n",
      "Reconstruction Loss:  0.08356494456529617\n",
      "Cyclic Loss:  0.3225446045398712\n",
      "Batch:  300\n",
      "Task Loss:  0.31685924530029297\n",
      "Reconstruction Loss:  0.08036432415246964\n",
      "Cyclic Loss:  0.3284403085708618\n",
      "Batch:  600\n",
      "Task Loss:  0.31615495681762695\n",
      "Reconstruction Loss:  0.12061254680156708\n",
      "Cyclic Loss:  0.68206387758255\n",
      "Batch:  900\n",
      "Task Loss:  0.26583540439605713\n",
      "Reconstruction Loss:  0.08481066673994064\n",
      "Cyclic Loss:  0.29548367857933044\n",
      "Batch:  1200\n",
      "Task Loss:  0.23073366284370422\n",
      "Reconstruction Loss:  0.08782146871089935\n",
      "Cyclic Loss:  0.2880465090274811\n",
      "Batch:  1500\n",
      "Task Loss:  0.23870296776294708\n",
      "Reconstruction Loss:  0.08930661529302597\n",
      "Cyclic Loss:  0.33710813522338867\n",
      "Elapsed Time:  1572.352025437355\n"
     ]
    }
   ],
   "source": [
    "loss_matrix=train(Encoder,phi,invphi,train_load,num_epochs=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
