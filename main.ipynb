{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import EncoderNet,DecoderNet,DiscriminatorNet_reconstruction,GeneratorNet\n",
    "from network import transformNet\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "#import resnet\n",
    "#import invresnet\n",
    "from dataload import load_data ,batchfy \n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms, utils\n",
    "TIMEOUT=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Decoder_64batch.h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1b96be7abc14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Encoder_64batch.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDecoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Decoder_64batch.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Encoder=resnet.resnet18()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Encoder=Encoder.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Decoder_64batch.h'"
     ]
    }
   ],
   "source": [
    "Encoder=torch.load(\"Encoder_64batch.h\")\n",
    "Decoder=torch.load(\"Decoder_64batch.h\")\n",
    "#Encoder=resnet.resnet18()\n",
    "#Encoder=Encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load,test_load=batchfy(batch_size=100)\n",
    "show_img=iter(train_load)\n",
    "for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "    debug=real_images[0]\n",
    "    plt.imshow((debug.numpy().transpose((1, 2, 0))*0.5)+0.5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phi(\n",
       "  (fc1): Linear(in_features=99, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=99, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi=transformNet.phi()\n",
    "phi.to(device)\n",
    "\n",
    "invphi=transformNet.phi(inv=True)\n",
    "invphi.to(device)\n",
    "\n",
    "#Transform=transformNet.full_phi(phi,invphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_phi = optim.Adam(phi.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "opt_invphi = optim.Adam(invphi.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [phi.parameters(), invphi.parameters()]\n",
    "\n",
    "opt_transform=optim.Adam(itertools.chain(*params),lr=0.001,betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripleletLoss(batch,targetAttribute):\n",
    "    def triplet(value, positive, negative, margin=0.2) : \n",
    "        d = nn.PairwiseDistance(p=2)\n",
    "        distance = d(value, positive) - d(value, negative) + margin \n",
    "        loss = torch.mean(torch.max(distance, torch.zeros_like(distance))) \n",
    "        return loss\n",
    "    \n",
    "    def findtriplet(src,attribute):\n",
    "        timeout_start = time.time()\n",
    "        index_list=np.arange(len(attribute)).tolist()\n",
    "        rand=random.sample(index_list,len(attribute))\n",
    "        for i,posindex in enumerate(rand):\n",
    "            if attribute[src]==attribute[posindex]:\n",
    "                if src != posindex:\n",
    "                        break      \n",
    "            if i==len(attribute)-1:\n",
    "                posindex=src            \n",
    "        rand=random.sample(index_list,len(attribute))                \n",
    "        for i,negindex in enumerate(rand):\n",
    "            if(attribute[src] !=attribute[negindex]):\n",
    "                break   \n",
    "            if i==len(attribute)-1:\n",
    "                negindex=src\n",
    "                \n",
    "        return posindex,negindex\n",
    "    loss=0\n",
    "    pos_pair=None\n",
    "    for i,value in enumerate(batch):\n",
    "        posindex,negindex=findtriplet(i,targetAttribute)\n",
    "\n",
    "        if not i:\n",
    "\n",
    "            pos_pair=batch[posindex].unsqueeze(0)\n",
    "            neg_pair=batch[negindex].unsqueeze(0)\n",
    "        else:\n",
    "            pos_pair=torch.cat((pos_pair,batch[posindex].unsqueeze(0)),0)\n",
    "            neg_pair=torch.cat((neg_pair,batch[negindex].unsqueeze(0)),0)\n",
    "\n",
    "\n",
    "    return triplet(batch,pos_pair,neg_pair)\n",
    "\n",
    "def reconstruction_loss(z,z_tilde,optimizer):\n",
    "    loss = nn.L1Loss()\n",
    "    optimizer.zero_grad()\n",
    "    error_recons=loss(z,z_tilde)\n",
    "    error_recons.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return error_recons\n",
    "\n",
    "def concat(z_list):\n",
    "    return torch.cat((z_list[0],z_list[1],z_list[2]),1)\n",
    "\n",
    "def cyclic_loss(z1,z2,z3,true_glasses,true_gender,opt_transform):\n",
    "    batch_size=z1.size(0)\n",
    "    swapped_pos=torch.randperm(batch_size)   \n",
    "    z1_hat = z1[swapped_pos]   #Permutation\n",
    "    true_glasses=true_glasses[swapped_pos]  \n",
    "    swapped_pos=torch.randperm(batch_size)\n",
    "    true_gender=true_glasses[swapped_pos]\n",
    "    z2_hat=z2[swapped_pos]\n",
    "    swapped_pos=torch.randperm(batch_size)\n",
    "    z3_hat=z3[swapped_pos]\n",
    "    true_gender=true_glasses[true_glasses]\n",
    "    z_aster=torch.cat((z1_hat,z2_hat,z3),1)\n",
    "    recontructed_z_aster=concat(phi(Encoder(Decoder(invphi(z_aster)))))\n",
    "    \n",
    "\n",
    "    \n",
    "    #Cycle_Consistency,Loss                 \n",
    "    opt_transform.zero_grad()\n",
    "    loss = nn.MSELoss()                                                     \n",
    "    consistency_loss = loss(z_aster,recontructed_z_aster)\n",
    "    consistency_loss.backward(retain_graph=True)\n",
    "    opt_transform.step()\n",
    "    \n",
    "    \n",
    "    #attr_cycle_augmentation_loss\n",
    "    opt_transform.zero_grad()\n",
    "    augmentation_loss =TripleletLoss(z1_hat,true_glasses) + TripleletLoss(z2_hat,true_gender)\n",
    "    augmentation_loss.backward()\n",
    "    opt_transform.step()\n",
    " \n",
    "    return consistency_loss +augmentation_loss       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [

=======
    "def train(Encoder,phi,invphi,train_load,num_epochs=40):\n",
    "    loss_matrix=None\n",
    "    first=True\n",
>>>>>>> 718235a810c6422927929ce9f8c4a490516ea543
    "    t_start = time.time()\n",
    "    Encoder.eval()\n",
    "    Decoder.eval()\n",
    "    phi.train()\n",
    "    invphi.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            latent_vector=Encoder(real_images).detach()\n",
    "            glass_vector,gender_vector,remain=phi(latent_vector)\n",
    "            \n",
    "            #Reconstruction Loss\n",
    "            z_tilde=invphi(torch.cat((glass_vector,gender_vector,remain),1))\n",
    "            loss_reconstruction=reconstruction_loss(latent_vector,z_tilde,opt_transform)\n",
    "            \n",
    "            \n",
    "            #Task Loss\n",
    "            opt_phi.zero_grad()       \n",
    "            loss=TripleletLoss(glass_vector,glasses) +    TripleletLoss(gender_vector,gender)  \n",
    "            loss.backward(retain_graph=True)\n",
    "            opt_phi.step()\n",
    "            \n",
    "            #glass_vector=glass_vector.detach()\n",
    "           # gender_vector=gender_vector.detach()\n",
    "           # remain=remain.detach()\n",
    "\n",
    "            \n",
    "            #Cyclic Loss\n",
    "            loss_cycle=cyclic_loss(glass_vector,gender_vector,remain,glasses,gender,opt_transform)\n",
    "            \n",
    "            \n",
    "            if (batch_i) % 300 == 0:\n",
    "                print(\"Batch: \", batch_i)\n",
    "                print(\"Task Loss: \", loss.item())\n",
    "                print(\"Reconstruction Loss: \",loss_reconstruction.item())\n",
    "                print(\"Cyclic Loss: \",loss_cycle.item())\n",
    "                if first:\n",
    "                    loss_matrix=np.array((loss.item(),loss_reconstruction.item(),loss_cycle.item()))\n",
    "                    first=False\n",
    "                else:\n",
    "                    loss_matrix=np.vstack((loss_matrix,np.array((loss.item(),loss_reconstruction.item(),loss_cycle.item()))))\n",
    "        t_end = time.time()\n",
    "        duration_avg = (t_end - t_start) / (epoch + 1.0)\n",
    "        print(\"Elapsed Time: \",duration_avg)\n",

=======
    "        torch.save(phi,'Phi.h')\n",
    "        torch.save(invphi,'invphi.h')\n",
    "    return loss_matrix"
>>>>>>> 718235a810c6422927929ce9f8c4a490516ea543
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch:  0\n",
      "Task Loss:  0.41485169529914856\n",
      "Reconstruction Loss:  0.4043228328227997\n",
      "Cyclic Loss:  1.5709236860275269\n",
      "Batch:  300\n",
      "Task Loss:  0.32545140385627747\n",
      "Reconstruction Loss:  0.10201364755630493\n",
      "Cyclic Loss:  0.5533437132835388\n",
      "Batch:  600\n",
      "Task Loss:  0.30165576934814453\n",
      "Reconstruction Loss:  0.10666044056415558\n",
      "Cyclic Loss:  0.43101686239242554\n",
      "Batch:  900\n",
      "Task Loss:  0.22351883351802826\n",
      "Reconstruction Loss:  0.10000026971101761\n",
      "Cyclic Loss:  0.3561117649078369\n",
      "Batch:  1200\n",
      "Task Loss:  0.3101048171520233\n",
      "Reconstruction Loss:  0.10411772131919861\n",
      "Cyclic Loss:  0.4668482840061188\n",
      "Batch:  1500\n",
      "Task Loss:  0.29572728276252747\n",
      "Reconstruction Loss:  0.10546872764825821\n",
      "Cyclic Loss:  0.349165141582489\n",
      "Elapsed Time:  1570.3096861839294\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type phi. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Task Loss:  0.2934541702270508\n",
      "Reconstruction Loss:  0.10784789174795151\n",
      "Cyclic Loss:  0.46450483798980713\n",
      "Batch:  300\n",
      "Task Loss:  0.23126842081546783\n",
      "Reconstruction Loss:  0.09972812235355377\n",
      "Cyclic Loss:  0.31033873558044434\n",
      "Batch:  600\n",
      "Task Loss:  0.29877969622612\n",
      "Reconstruction Loss:  0.09143738448619843\n",
      "Cyclic Loss:  0.3066519498825073\n",
      "Batch:  900\n",
      "Task Loss:  0.27809929847717285\n",
      "Reconstruction Loss:  0.09516167640686035\n",
      "Cyclic Loss:  0.35675376653671265\n",
      "Batch:  1200\n",
      "Task Loss:  0.26769736409187317\n",
      "Reconstruction Loss:  0.10073316842317581\n",
      "Cyclic Loss:  0.42011886835098267\n",
      "Batch:  1500\n",
      "Task Loss:  0.29206162691116333\n",
      "Reconstruction Loss:  0.09946095943450928\n",
      "Cyclic Loss:  0.24799978733062744\n",
      "Elapsed Time:  1569.5283200740814\n",
      "Epoch: 2\n",
      "Batch:  0\n",
      "Task Loss:  0.2703222930431366\n",
      "Reconstruction Loss:  0.0904562696814537\n",
      "Cyclic Loss:  0.31055641174316406\n",
      "Batch:  300\n",
      "Task Loss:  0.2664741575717926\n",
      "Reconstruction Loss:  0.08968904614448547\n",
      "Cyclic Loss:  0.33835259079933167\n",
      "Batch:  600\n",
      "Task Loss:  0.26454028487205505\n",
      "Reconstruction Loss:  0.11322079598903656\n",
      "Cyclic Loss:  0.5295050144195557\n",
      "Batch:  900\n",
      "Task Loss:  0.21743862330913544\n",
      "Reconstruction Loss:  0.09974829107522964\n",
      "Cyclic Loss:  0.43126049637794495\n",
      "Batch:  1200\n",
      "Task Loss:  0.24467605352401733\n",
      "Reconstruction Loss:  0.09193921834230423\n",
      "Cyclic Loss:  0.2610914707183838\n",
      "Batch:  1500\n",
      "Task Loss:  0.2418588399887085\n",
      "Reconstruction Loss:  0.09427006542682648\n",
      "Cyclic Loss:  0.339705228805542\n",
      "Elapsed Time:  1576.4744562307994\n",
      "Epoch: 3\n",
      "Batch:  0\n",
      "Task Loss:  0.3236810564994812\n",
      "Reconstruction Loss:  0.08453822880983353\n",
      "Cyclic Loss:  0.3274063766002655\n",
      "Batch:  300\n",
      "Task Loss:  0.23834745585918427\n",
      "Reconstruction Loss:  0.08753841370344162\n",
      "Cyclic Loss:  0.2965394854545593\n",
      "Batch:  600\n",
      "Task Loss:  0.2471085637807846\n",
      "Reconstruction Loss:  0.09782013297080994\n",
      "Cyclic Loss:  0.38341212272644043\n",
      "Batch:  900\n",
      "Task Loss:  0.26681140065193176\n",
      "Reconstruction Loss:  0.09172187000513077\n",
      "Cyclic Loss:  0.34337252378463745\n",
      "Batch:  1200\n",
      "Task Loss:  0.27790942788124084\n",
      "Reconstruction Loss:  0.08707629889249802\n",
      "Cyclic Loss:  0.31561020016670227\n",
      "Batch:  1500\n",
      "Task Loss:  0.4012511074542999\n",
      "Reconstruction Loss:  0.1608409434556961\n",
      "Cyclic Loss:  0.9621202349662781\n",
      "Elapsed Time:  1578.2365495562553\n",
      "Epoch: 4\n",
      "Batch:  0\n",
      "Task Loss:  0.23541998863220215\n",
      "Reconstruction Loss:  0.08466091752052307\n",
      "Cyclic Loss:  0.26862165331840515\n",
      "Batch:  300\n",
      "Task Loss:  0.35048753023147583\n",
      "Reconstruction Loss:  0.11812318116426468\n",
      "Cyclic Loss:  0.5745354294776917\n",
      "Batch:  600\n",
      "Task Loss:  0.29838764667510986\n",
      "Reconstruction Loss:  0.08857832103967667\n",
      "Cyclic Loss:  0.34937772154808044\n",
      "Batch:  900\n",
      "Task Loss:  0.27876636385917664\n",
      "Reconstruction Loss:  0.08690991997718811\n",
      "Cyclic Loss:  0.28952041268348694\n",
      "Batch:  1200\n",
      "Task Loss:  0.27566248178482056\n",
      "Reconstruction Loss:  0.08816554397344589\n",
      "Cyclic Loss:  0.3400076627731323\n",
      "Batch:  1500\n",
      "Task Loss:  0.3225793242454529\n",
      "Reconstruction Loss:  0.08741609752178192\n",
      "Cyclic Loss:  0.2988104820251465\n",
      "Elapsed Time:  1575.341585111618\n",
      "Epoch: 5\n",
      "Batch:  0\n",
      "Task Loss:  0.2533304989337921\n",
      "Reconstruction Loss:  0.10903581976890564\n",
      "Cyclic Loss:  0.5581241250038147\n",
      "Batch:  300\n",
      "Task Loss:  0.26907819509506226\n",
      "Reconstruction Loss:  0.08352721482515335\n",
      "Cyclic Loss:  0.34064820408821106\n",
      "Batch:  600\n",
      "Task Loss:  0.2604994773864746\n",
      "Reconstruction Loss:  0.10409679263830185\n",
      "Cyclic Loss:  0.2781238555908203\n",
      "Batch:  900\n",
      "Task Loss:  0.24537846446037292\n",
      "Reconstruction Loss:  0.0859021544456482\n",
      "Cyclic Loss:  0.2784212827682495\n",
      "Batch:  1200\n",
      "Task Loss:  0.29180628061294556\n",
      "Reconstruction Loss:  0.0896877646446228\n",
      "Cyclic Loss:  0.4060460031032562\n",
      "Batch:  1500\n",
      "Task Loss:  0.26940053701400757\n",
      "Reconstruction Loss:  0.10357245802879333\n",
      "Cyclic Loss:  0.42689424753189087\n",
      "Elapsed Time:  1578.04711886247\n",
      "Epoch: 6\n",
      "Batch:  0\n",
      "Task Loss:  0.2547900080680847\n",
      "Reconstruction Loss:  0.08693521469831467\n",
      "Cyclic Loss:  0.3013344407081604\n",
      "Batch:  300\n",
      "Task Loss:  0.31309032440185547\n",
      "Reconstruction Loss:  0.08387365192174911\n",
      "Cyclic Loss:  0.3281097114086151\n",
      "Batch:  600\n",
      "Task Loss:  0.2989406883716583\n",
      "Reconstruction Loss:  0.1066598892211914\n",
      "Cyclic Loss:  0.4234801232814789\n",
      "Batch:  900\n",
      "Task Loss:  0.31490445137023926\n",
      "Reconstruction Loss:  0.08155510574579239\n",
      "Cyclic Loss:  0.28526005148887634\n",
      "Batch:  1200\n",
      "Task Loss:  0.25138694047927856\n",
      "Reconstruction Loss:  0.08548209071159363\n",
      "Cyclic Loss:  0.30265942215919495\n",
      "Batch:  1500\n",
      "Task Loss:  0.27096474170684814\n",
      "Reconstruction Loss:  0.09916459769010544\n",
      "Cyclic Loss:  0.4114244878292084\n",
      "Elapsed Time:  1576.4791998182025\n",
      "Epoch: 7\n",
      "Batch:  0\n",
      "Task Loss:  0.27287623286247253\n",
      "Reconstruction Loss:  0.10037808120250702\n",
      "Cyclic Loss:  0.4009259045124054\n",
      "Batch:  300\n",
      "Task Loss:  0.2510908544063568\n",
      "Reconstruction Loss:  0.09709589183330536\n",
      "Cyclic Loss:  0.40709036588668823\n",
      "Batch:  600\n",
      "Task Loss:  0.19621002674102783\n",
      "Reconstruction Loss:  0.10703279823064804\n",
      "Cyclic Loss:  0.3601485788822174\n",
      "Batch:  900\n",
      "Task Loss:  0.2809951603412628\n",
      "Reconstruction Loss:  0.09336381405591965\n",
      "Cyclic Loss:  0.4100351929664612\n",
      "Batch:  1200\n",
      "Task Loss:  0.22097228467464447\n",
      "Reconstruction Loss:  0.09617336839437485\n",
      "Cyclic Loss:  0.3553208112716675\n",
      "Batch:  1500\n",
      "Task Loss:  0.2378058284521103\n",
      "Reconstruction Loss:  0.09232481569051743\n",
      "Cyclic Loss:  0.3386535048484802\n",
      "Elapsed Time:  1575.6111213564873\n",
      "Epoch: 8\n",
      "Batch:  0\n",
      "Task Loss:  0.28810277581214905\n",
      "Reconstruction Loss:  0.09333808720111847\n",
      "Cyclic Loss:  0.4269362986087799\n",
      "Batch:  300\n",
      "Task Loss:  0.2675704061985016\n",
      "Reconstruction Loss:  0.08417532593011856\n",
      "Cyclic Loss:  0.3240911066532135\n",
      "Batch:  600\n",
      "Task Loss:  0.251505970954895\n",
      "Reconstruction Loss:  0.08453690260648727\n",
      "Cyclic Loss:  0.2657924294471741\n"
     ]
    }
   ],
   "source": [
    "loss_matrix=train(Encoder,phi,invphi,train_load,num_epochs=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
