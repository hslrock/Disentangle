{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import EncoderNet,DecoderNet,DiscriminatorNet_reconstruction,GeneratorNet\n",
    "from network import transformNet\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "#import resnet\n",
    "#import invresnet\n",
    "from dataload import load_data ,batchfy \n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms, utils\n",
    "TIMEOUT=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Decoder_64batch.h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1b96be7abc14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Encoder_64batch.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDecoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Decoder_64batch.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Encoder=resnet.resnet18()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Encoder=Encoder.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Decoder_64batch.h'"
     ]
    }
   ],
   "source": [
    "Encoder=torch.load(\"Encoder_64batch.h\")\n",
    "Decoder=torch.load(\"Decoder_64batch.h\")\n",
    "#Encoder=resnet.resnet18()\n",
    "#Encoder=Encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load,test_load=batchfy(batch_size=100)\n",
    "show_img=iter(train_load)\n",
    "for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "    debug=real_images[0]\n",
    "    plt.imshow((debug.numpy().transpose((1, 2, 0))*0.5)+0.5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phi(\n",
       "  (fc1): Linear(in_features=99, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=99, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi=transformNet.phi()\n",
    "phi.to(device)\n",
    "\n",
    "invphi=transformNet.phi(inv=True)\n",
    "invphi.to(device)\n",
    "\n",
    "#Transform=transformNet.full_phi(phi,invphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_phi = optim.Adam(phi.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "opt_invphi = optim.Adam(invphi.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [phi.parameters(), invphi.parameters()]\n",
    "\n",
    "opt_transform=optim.Adam(itertools.chain(*params),lr=0.001,betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripleletLoss(batch,targetAttribute):\n",
    "    def triplet(value, positive, negative, margin=0.2) : \n",
    "        d = nn.PairwiseDistance(p=2)\n",
    "        distance = d(value, positive) - d(value, negative) + margin \n",
    "        loss = torch.mean(torch.max(distance, torch.zeros_like(distance))) \n",
    "        return loss\n",
    "    \n",
    "    def findtriplet(src,attribute):\n",
    "        timeout_start = time.time()\n",
    "        index_list=np.arange(len(attribute)).tolist()\n",
    "        rand=random.sample(index_list,len(attribute))\n",
    "        for i,posindex in enumerate(rand):\n",
    "            if attribute[src]==attribute[posindex]:\n",
    "                if src != posindex:\n",
    "                        break      \n",
    "            if i==len(attribute)-1:\n",
    "                posindex=src            \n",
    "        rand=random.sample(index_list,len(attribute))                \n",
    "        for i,negindex in enumerate(rand):\n",
    "            if(attribute[src] !=attribute[negindex]):\n",
    "                break   \n",
    "            if i==len(attribute)-1:\n",
    "                negindex=src\n",
    "                \n",
    "        return posindex,negindex\n",
    "    loss=0\n",
    "    pos_pair=None\n",
    "    for i,value in enumerate(batch):\n",
    "        posindex,negindex=findtriplet(i,targetAttribute)\n",
    "\n",
    "        if not i:\n",
    "\n",
    "            pos_pair=batch[posindex].unsqueeze(0)\n",
    "            neg_pair=batch[negindex].unsqueeze(0)\n",
    "        else:\n",
    "            pos_pair=torch.cat((pos_pair,batch[posindex].unsqueeze(0)),0)\n",
    "            neg_pair=torch.cat((neg_pair,batch[negindex].unsqueeze(0)),0)\n",
    "\n",
    "\n",
    "    return triplet(batch,pos_pair,neg_pair)\n",
    "\n",
    "def reconstruction_loss(z,z_tilde,optimizer):\n",
    "    loss = nn.L1Loss()\n",
    "    optimizer.zero_grad()\n",
    "    error_recons=loss(z,z_tilde)\n",
    "    error_recons.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return error_recons\n",
    "\n",
    "def concat(z_list):\n",
    "    return torch.cat((z_list[0],z_list[1],z_list[2]),1)\n",
    "\n",
    "def cyclic_loss(z1,z2,z3,true_glasses,true_gender,opt_transform):\n",
    "    batch_size=z1.size(0)\n",
    "    swapped_pos=torch.randperm(batch_size)   \n",
    "    z1_hat = z1[swapped_pos]   #Permutation\n",
    "    true_glasses=true_glasses[swapped_pos]  \n",
    "    swapped_pos=torch.randperm(batch_size)\n",
    "    true_gender=true_glasses[swapped_pos]\n",
    "    z2_hat=z2[swapped_pos]\n",
    "    swapped_pos=torch.randperm(batch_size)\n",
    "    z3_hat=z3[swapped_pos]\n",
    "    true_gender=true_glasses[true_glasses]\n",
    "    z_aster=torch.cat((z1_hat,z2_hat,z3),1)\n",
    "    recontructed_z_aster=concat(phi(Encoder(Decoder(invphi(z_aster)))))\n",
    "    #Cycle_Consistency,Loss                 \n",
    "    opt_transform.zero_grad()\n",
    "    loss = nn.MSELoss()                                                     \n",
    "    consistency_loss = loss(z_aster,recontructed_z_aster)\n",
    "    consistency_loss.backward()\n",
    "    opt_transform.step()\n",
    "    \n",
    "    \n",
    "   # opt_transform.zero_grad()\n",
    "    #augmentation_loss =TripleletLoss(z1_hat,true_glasses)# + TripleletLoss(z2_hat,true_gender)\n",
    "    #print(augmentation_loss)\n",
    "  #  augmenstation_loss.backward()\n",
    "   # opt_transform.step()\n",
    "    return consistency_loss #+augmentation_loss       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(Encoder,phi,invphi,train_load,num_epochs=40):\n",
    "    t_start = time.time()\n",
    "    Encoder.eval()\n",
    "    Decoder.eval()\n",
    "    phi.train()\n",
    "    invphi.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batch_i, (real_images, gender,glasses) in enumerate(train_load):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images=real_images.to(device,dtype=torch.float)\n",
    "            latent_vector=Encoder(real_images).detach()\n",
    "            glass_vector,gender_vector,remain=phi(latent_vector)\n",
    "            \n",
    "            #Reconstruction Loss\n",
    "            z_tilde=invphi(torch.cat((glass_vector,gender_vector,remain),1))\n",
    "            loss_reconstruction=reconstruction_loss(latent_vector,z_tilde,opt_transform)\n",
    "            \n",
    "            \n",
    "            #Task Loss\n",
    "            opt_phi.zero_grad()       \n",
    "            loss=TripleletLoss(glass_vector,glasses) +    TripleletLoss(gender_vector,gender)  \n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            opt_phi.step()\n",
    "            \n",
    "            glass_vector=glass_vector.detach()\n",
    "            gender_vector=gender_vector.detach()\n",
    "            remain=remain.detach()\n",
    "\n",
    "            \n",
    "            #Cyclic Loss\n",
    "            loss_cycle=cyclic_loss(glass_vector,gender_vector,remain,glasses,gender,opt_transform)\n",
    "            \n",
    "            \n",
    "            if (batch_i) % 300 == 0:\n",
    "                print(\"Batch: \", batch_i)\n",
    "                print(\"Task Loss: \", loss.item())\n",
    "                print(\"Reconstruction Loss: \",loss_reconstruction.item())\n",
    "                print(\"Cyclic Loss: \",loss_cycle.item())\n",
    "        t_end = time.time()\n",
    "        duration_avg = (t_end - t_start) / (epoch + 1.0)\n",
    "        print(\"Elapsed Time: \",duration_avg)\n",
    "        #torch.save(phi,'Phi.h')\n",
    "        break\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(Encoder,phi,invphi,train_load,num_epochs=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.randperm(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
